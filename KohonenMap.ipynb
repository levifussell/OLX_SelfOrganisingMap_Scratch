{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/levi/virtualenvs/iaml_env/local/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_time</th>\n",
       "      <th>user_id</th>\n",
       "      <th>event</th>\n",
       "      <th>channel</th>\n",
       "      <th>user_lat</th>\n",
       "      <th>user_long</th>\n",
       "      <th>origin</th>\n",
       "      <th>ad_id</th>\n",
       "      <th>images_count</th>\n",
       "      <th>ad_impressions</th>\n",
       "      <th>ad_views</th>\n",
       "      <th>ad_messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-05-19 10:47:22</td>\n",
       "      <td>541</td>\n",
       "      <td>view</td>\n",
       "      <td>ios</td>\n",
       "      <td>-34.600439</td>\n",
       "      <td>-58.514031</td>\n",
       "      <td>browse_search</td>\n",
       "      <td>2426321</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-06-10 10:18:12</td>\n",
       "      <td>501</td>\n",
       "      <td>view</td>\n",
       "      <td>android</td>\n",
       "      <td>-34.818047</td>\n",
       "      <td>-58.356583</td>\n",
       "      <td>browse_search</td>\n",
       "      <td>2746814</td>\n",
       "      <td>5.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-06-11 00:44:57</td>\n",
       "      <td>559</td>\n",
       "      <td>view</td>\n",
       "      <td>android</td>\n",
       "      <td>-34.688454</td>\n",
       "      <td>-58.333435</td>\n",
       "      <td>browse</td>\n",
       "      <td>1123948</td>\n",
       "      <td>4.0</td>\n",
       "      <td>899.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-05-22 02:36:06</td>\n",
       "      <td>637</td>\n",
       "      <td>first_message</td>\n",
       "      <td>android</td>\n",
       "      <td>-34.548401</td>\n",
       "      <td>-58.485168</td>\n",
       "      <td>search</td>\n",
       "      <td>2273498</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-06-08 15:45:35</td>\n",
       "      <td>509</td>\n",
       "      <td>view</td>\n",
       "      <td>android</td>\n",
       "      <td>-34.582088</td>\n",
       "      <td>-58.405041</td>\n",
       "      <td>home</td>\n",
       "      <td>2722493</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1529.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            event_time  user_id          event  channel   user_lat  user_long  \\\n",
       "0  2017-05-19 10:47:22      541           view      ios -34.600439 -58.514031   \n",
       "1  2017-06-10 10:18:12      501           view  android -34.818047 -58.356583   \n",
       "2  2017-06-11 00:44:57      559           view  android -34.688454 -58.333435   \n",
       "3  2017-05-22 02:36:06      637  first_message  android -34.548401 -58.485168   \n",
       "4  2017-06-08 15:45:35      509           view  android -34.582088 -58.405041   \n",
       "\n",
       "          origin    ad_id  images_count  ad_impressions  ad_views  ad_messages  \n",
       "0  browse_search  2426321           6.0             0.0       4.0          0.0  \n",
       "1  browse_search  2746814           5.0           152.0      85.0          1.0  \n",
       "2         browse  1123948           4.0           899.0      22.0          1.0  \n",
       "3         search  2273498           NaN             0.0      21.0          2.0  \n",
       "4           home  2722493           3.0          1529.0     132.0          3.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_dt = pd.read_csv('Data/user_data.csv', sep=',', header=0)\n",
    "user_dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ad_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>seller_id</th>\n",
       "      <th>creation_time</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>source</th>\n",
       "      <th>enabled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1344</td>\n",
       "      <td>800</td>\n",
       "      <td>1157</td>\n",
       "      <td>2015-11-29 13:05:26</td>\n",
       "      <td>Blackberry curve</td>\n",
       "      <td>Funcionando, es para personal el único problem...</td>\n",
       "      <td>300.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>android</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3916</td>\n",
       "      <td>815</td>\n",
       "      <td>59</td>\n",
       "      <td>2015-12-16 09:09:25</td>\n",
       "      <td>Remera marca premium alemana</td>\n",
       "      <td>Casi nuevo y sin uso. Lo compre online y me qu...</td>\n",
       "      <td>150.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>android</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11738</td>\n",
       "      <td>806</td>\n",
       "      <td>12298</td>\n",
       "      <td>2016-01-19 18:38:24</td>\n",
       "      <td>Juego de sillones estilo campo</td>\n",
       "      <td>Divino juego de sillones de diseño campo</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>android</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14939</td>\n",
       "      <td>815</td>\n",
       "      <td>1349</td>\n",
       "      <td>2016-01-30 23:47:30</td>\n",
       "      <td>Zapatillas</td>\n",
       "      <td>Zapatillas nro 39 dos usos</td>\n",
       "      <td>250.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>android</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16820</td>\n",
       "      <td>815</td>\n",
       "      <td>16365</td>\n",
       "      <td>2016-02-05 10:22:29</td>\n",
       "      <td>Vestido de seda negro</td>\n",
       "      <td>Vestido importado de usa. Muy fino y elegante ...</td>\n",
       "      <td>500.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>android</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ad_id  category_id  seller_id        creation_time  \\\n",
       "0   1344          800       1157  2015-11-29 13:05:26   \n",
       "1   3916          815         59  2015-12-16 09:09:25   \n",
       "2  11738          806      12298  2016-01-19 18:38:24   \n",
       "3  14939          815       1349  2016-01-30 23:47:30   \n",
       "4  16820          815      16365  2016-02-05 10:22:29   \n",
       "\n",
       "                            title  \\\n",
       "0                Blackberry curve   \n",
       "1    Remera marca premium alemana   \n",
       "2  Juego de sillones estilo campo   \n",
       "3                      Zapatillas   \n",
       "4           Vestido de seda negro   \n",
       "\n",
       "                                         description   price  lat  long  \\\n",
       "0  Funcionando, es para personal el único problem...   300.0  NaN   NaN   \n",
       "1  Casi nuevo y sin uso. Lo compre online y me qu...   150.0  NaN   NaN   \n",
       "2           Divino juego de sillones de diseño campo  1500.0  NaN   NaN   \n",
       "3                         Zapatillas nro 39 dos usos   250.0  NaN   NaN   \n",
       "4  Vestido importado de usa. Muy fino y elegante ...   500.0  NaN   NaN   \n",
       "\n",
       "    source  enabled  \n",
       "0  android        0  \n",
       "1  android        1  \n",
       "2  android        1  \n",
       "3  android        0  \n",
       "4  android        0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ads_dt = pd.read_csv('Data/ads_data.csv', sep=',', header=0)\n",
    "ads_dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>ads</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>859</td>\n",
       "      <td>[1806476]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>800</td>\n",
       "      <td>[2131700, 2734107, 2877209, 2877209]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>815</td>\n",
       "      <td>[2883211]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>815</td>\n",
       "      <td>[2429412, 2886810, 2886804]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>362</td>\n",
       "      <td>[2909301]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  category_id                                   ads\n",
       "0        1          859                             [1806476]\n",
       "1        3          800  [2131700, 2734107, 2877209, 2877209]\n",
       "2        4          815                             [2883211]\n",
       "3        7          815           [2429412, 2886810, 2886804]\n",
       "4        7          362                             [2909301]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ad_interests_dt = pd.read_csv('Data/user_messages.csv', sep=',', header=0)\n",
    "user_ad_interests_dt.head()\n",
    "# remember this is the ads a user went to (in a 7 day period) after the 30 days prior of the user_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = ads_dt['category_id'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing Ads with most messages ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ad_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>seller_id</th>\n",
       "      <th>creation_time</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>source</th>\n",
       "      <th>...</th>\n",
       "      <th>user_id</th>\n",
       "      <th>event</th>\n",
       "      <th>channel</th>\n",
       "      <th>user_lat</th>\n",
       "      <th>user_long</th>\n",
       "      <th>origin</th>\n",
       "      <th>images_count</th>\n",
       "      <th>ad_impressions</th>\n",
       "      <th>ad_views</th>\n",
       "      <th>ad_messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1918128</th>\n",
       "      <td>2804693</td>\n",
       "      <td>806</td>\n",
       "      <td>434981</td>\n",
       "      <td>2017-06-07 19:54:42</td>\n",
       "      <td>planta de limon</td>\n",
       "      <td>planta de limón</td>\n",
       "      <td>149.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>apple</td>\n",
       "      <td>...</td>\n",
       "      <td>1966</td>\n",
       "      <td>view</td>\n",
       "      <td>android</td>\n",
       "      <td>-34.543266</td>\n",
       "      <td>-58.692127</td>\n",
       "      <td>browse_search</td>\n",
       "      <td>1.0</td>\n",
       "      <td>693295.0</td>\n",
       "      <td>7165.0</td>\n",
       "      <td>189.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337225</th>\n",
       "      <td>2814436</td>\n",
       "      <td>800</td>\n",
       "      <td>635875</td>\n",
       "      <td>2017-06-08 22:30:32</td>\n",
       "      <td>iPhone 6 en excelente estado !</td>\n",
       "      <td>iPhone 6 en excelente estado !! muy poco uso.c...</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>apple</td>\n",
       "      <td>...</td>\n",
       "      <td>4171</td>\n",
       "      <td>view</td>\n",
       "      <td>android</td>\n",
       "      <td>-34.581360</td>\n",
       "      <td>-58.420368</td>\n",
       "      <td>search</td>\n",
       "      <td>6.0</td>\n",
       "      <td>205481.0</td>\n",
       "      <td>3696.0</td>\n",
       "      <td>118.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1513127</th>\n",
       "      <td>2726025</td>\n",
       "      <td>815</td>\n",
       "      <td>490708</td>\n",
       "      <td>2017-05-28 22:41:53</td>\n",
       "      <td>Buzo abrigado abercrombie</td>\n",
       "      <td>Muy abrigado, Azul, peludito adentro, letras p...</td>\n",
       "      <td>260.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>apple</td>\n",
       "      <td>...</td>\n",
       "      <td>5751</td>\n",
       "      <td>view</td>\n",
       "      <td>android</td>\n",
       "      <td>-34.663368</td>\n",
       "      <td>-58.508793</td>\n",
       "      <td>notification_center</td>\n",
       "      <td>2.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>730.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417601</th>\n",
       "      <td>2347424</td>\n",
       "      <td>815</td>\n",
       "      <td>540480</td>\n",
       "      <td>2017-04-13 21:02:43</td>\n",
       "      <td>Campera cuero negra. Sin uso</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>android</td>\n",
       "      <td>...</td>\n",
       "      <td>10204</td>\n",
       "      <td>view</td>\n",
       "      <td>android</td>\n",
       "      <td>-34.645706</td>\n",
       "      <td>-58.570988</td>\n",
       "      <td>search</td>\n",
       "      <td>3.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>1122.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307236</th>\n",
       "      <td>2598159</td>\n",
       "      <td>362</td>\n",
       "      <td>335997</td>\n",
       "      <td>2017-05-14 10:47:19</td>\n",
       "      <td>Fiat 147 modelo 87</td>\n",
       "      <td>Papeles al dia</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>android</td>\n",
       "      <td>...</td>\n",
       "      <td>4622</td>\n",
       "      <td>view</td>\n",
       "      <td>android</td>\n",
       "      <td>-34.726551</td>\n",
       "      <td>-58.322262</td>\n",
       "      <td>browse_search</td>\n",
       "      <td>4.0</td>\n",
       "      <td>645.0</td>\n",
       "      <td>527.0</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ad_id  category_id  seller_id        creation_time  \\\n",
       "1918128  2804693          806     434981  2017-06-07 19:54:42   \n",
       "1337225  2814436          800     635875  2017-06-08 22:30:32   \n",
       "1513127  2726025          815     490708  2017-05-28 22:41:53   \n",
       "417601   2347424          815     540480  2017-04-13 21:02:43   \n",
       "1307236  2598159          362     335997  2017-05-14 10:47:19   \n",
       "\n",
       "                                  title  \\\n",
       "1918128                 planta de limon   \n",
       "1337225  iPhone 6 en excelente estado !   \n",
       "1513127       Buzo abrigado abercrombie   \n",
       "417601     Campera cuero negra. Sin uso   \n",
       "1307236              Fiat 147 modelo 87   \n",
       "\n",
       "                                               description    price  lat  \\\n",
       "1918128                                    planta de limón    149.0  NaN   \n",
       "1337225  iPhone 6 en excelente estado !! muy poco uso.c...   8000.0  NaN   \n",
       "1513127  Muy abrigado, Azul, peludito adentro, letras p...    260.0  NaN   \n",
       "417601                                                 NaN    500.0  NaN   \n",
       "1307236                                     Papeles al dia  12000.0  NaN   \n",
       "\n",
       "         long   source     ...       user_id event  channel   user_lat  \\\n",
       "1918128   NaN    apple     ...          1966  view  android -34.543266   \n",
       "1337225   NaN    apple     ...          4171  view  android -34.581360   \n",
       "1513127   NaN    apple     ...          5751  view  android -34.663368   \n",
       "417601    NaN  android     ...         10204  view  android -34.645706   \n",
       "1307236   NaN  android     ...          4622  view  android -34.726551   \n",
       "\n",
       "         user_long               origin  images_count ad_impressions  \\\n",
       "1918128 -58.692127        browse_search           1.0       693295.0   \n",
       "1337225 -58.420368               search           6.0       205481.0   \n",
       "1513127 -58.508793  notification_center           2.0           79.0   \n",
       "417601  -58.570988               search           3.0          257.0   \n",
       "1307236 -58.322262        browse_search           4.0          645.0   \n",
       "\n",
       "         ad_views  ad_messages  \n",
       "1918128    7165.0        189.0  \n",
       "1337225    3696.0        118.0  \n",
       "1513127     730.0         99.0  \n",
       "417601     1122.0         99.0  \n",
       "1307236     527.0         92.0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the ads with the most messages that are sorted and duplicates removed, find out how much of the messages\n",
    "#  are represented by 20% of the population:\n",
    "\n",
    "# remove all nan data\n",
    "user_dt_messages_clean = user_dt[user_dt['ad_messages'].isnull() == False]\n",
    "\n",
    "# merge the user data and the ad data\n",
    "user_dt_msg_with_ads_data = ads_dt.merge(user_dt_messages_clean, on='ad_id')\n",
    "user_dt_msg_with_ads_data_sort = user_dt_msg_with_ads_data.sort_values('ad_messages', ascending = False)\n",
    "user_dt_msg_with_ads_data_sort.head()\n",
    "\n",
    "# remove duplicates\n",
    "user_dt_msg_with_ads_data_sort_no_dups = user_dt_msg_with_ads_data_sort[user_dt_msg_with_ads_data_sort.duplicated(subset='ad_id', keep='first') == False]\n",
    "user_dt_msg_with_ads_data_sort_no_dups.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we are going to analyse if our data follows the Pareto distribution: that is if 20% of the population represents 80% of the total messages of the entire population** (OMG IT DOES FOLLOW THE PARETO DISTRIBUTION!!!!!!)\n",
    "\n",
    "For now, we will use a small percentage of the top ads in order for out model to be small and easily maneagable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of messages of top 20% of ads: 302069.0\n",
      "number of messages of top 100% of ads: 381938.0\n",
      "79.0884908022% of ad messages represented by top 20% of the ads (89407 ads)\n"
     ]
    }
   ],
   "source": [
    "twenty_percent_indx = int(len(user_dt_msg_with_ads_data_sort_no_dups) * 0.17)\n",
    "\n",
    "user_dt_ad_dt_twenty_percent = user_dt_msg_with_ads_data_sort_no_dups[0:twenty_percent_indx]\n",
    "num_messages_20p = sum(user_dt_ad_dt_twenty_percent['ad_messages'].values)\n",
    "num_messages_100p = sum(user_dt_msg_with_ads_data_sort_no_dups['ad_messages'].values)\n",
    "print(\"number of messages of top 20% of ads: {}\".format(num_messages_20p))\n",
    "print(\"number of messages of top 100% of ads: {}\".format(num_messages_100p))\n",
    "percent_rep_20p = 100.0 * num_messages_20p / num_messages_100p\n",
    "print(\"{}% of ad messages represented by top 20% of the ads ({} ads)\".format(percent_rep_20p, twenty_percent_indx))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Organising Map (Kohonen Map) ##\n",
    "** Using a SOM to create a recommendation system. The input vector is an 1xN vector of features where each feature f_i is an ad (the # of ads used is reduced to a small percentage (approx. 20%) of the top ads). f_i is either {0, 1} where 0 means the user did not message this ad and 1 means the user did message this ad. We feed this data into the SOM which has an internal structure of MxM nodes. After the SOM has been trained, a new user's recommendations can be determined by feeding its history of messaged ads into the SOM, finding the strongest neuron and then either: selecting from the ads that other users in that neuron also chose in the post-7-day period, OR choosing ads the user might message by looking at the weights of the neuron (i.e. these weights should correspond with ads that user WOULD choose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import exp\n",
    "from math import sqrt\n",
    "\n",
    "# a class for kohonen map\n",
    "class KohonenMap:\n",
    "    \n",
    "    def __init__(self, size_feature_vec, dim_neuron):\n",
    "        self.size_feature_vec = size_feature_vec\n",
    "        self.dim_neuron = dim_neuron\n",
    "        # neuron weights stored in an NxW matrix\n",
    "        #  where: N = num neurons, W = feature vec size\n",
    "        # --> weights randomly initialised\n",
    "#         self.weights = ((np.random.randn(self.dim_neuron * self.dim_neuron, self.size_feature_vec)))\n",
    "        # random binary initialisation\n",
    "        self.weights = ((np.random.randn(self.dim_neuron * self.dim_neuron, self.size_feature_vec) > 0.9).astype(int)).astype(float)\n",
    "        print('starting weights: \\n{}'.format(self.weights))\n",
    "        \n",
    "        # print x, y position of each neuron\n",
    "#         pos = []\n",
    "#         for i in range(0, self.dim_neuron * self.dim_neuron):\n",
    "#             pos.append(self.get_neuron_position(i))\n",
    "#         print(\"neuron pos: {}\".format(pos))\n",
    "                \n",
    "        # keep track of average updates of values to see\n",
    "        #  that the map is changing\n",
    "        self.avg_activation_val = 0.0\n",
    "        self.avg_dw_mean_update = np.zeros(self.size_feature_vec)\n",
    "        self.avg_dw_max_update = np.zeros(self.size_feature_vec)\n",
    "        \n",
    "        self.error_over_time = []\n",
    "        self.time_step = 0\n",
    "        \n",
    "        # assign hyperparams\n",
    "        self.var_0 = 3 #0.00001\n",
    "        self.learning_rate_0 = 1.0 #1.0\n",
    "        self.update_hyper_params()\n",
    "        \n",
    "        # dynamic testing\n",
    "        self.steps_until_param_write = 200 # how often to write out parameter data\n",
    "        self.steps_until_error_test = 600 # how often to test on the validation set\n",
    "\n",
    "        \n",
    "    def set_weights(self, loaded_weights):\n",
    "        self.weights = loaded_weights\n",
    "        print('starting weights: \\n{}'.format(self.weights))\n",
    "    \n",
    "    def set_time_step(self, time_step):\n",
    "        self.time_step = time_step\n",
    "        \n",
    "    # given the index of a neuron, get its \"position\" in 2d space\n",
    "    def get_neuron_position(self, neuron_idx):\n",
    "        y_pos = int(float(neuron_idx) / float(self.dim_neuron))\n",
    "        x_pos = neuron_idx % self.dim_neuron\n",
    "        return (x_pos, y_pos)\n",
    "    \n",
    "    def get_neuron_index(self, x_pos, y_pos):\n",
    "        return y_pos * self.dim_neuron + x_pos\n",
    "    \n",
    "    # return the index of the neuron with the heighest activation\n",
    "    #  (i.e. weights which are closest to the weights of the input)\n",
    "    def get_best_neuron(self, input_feature_vec):\n",
    "        if np.shape(input_feature_vec) != (1, self.size_feature_vec):\n",
    "            raise Exception(\"input feature vector {} is but should be {}\".format(np.shape(input_feature_vec), (1, self.size_feature_vec)))\n",
    "        \n",
    "        diff_sqr_sum = np.sum(np.power(self.weights - input_feature_vec[0, :], 2), axis=1)\n",
    "        #print(\"min dist: {}\".format(np.min(diff_sqr_sum)))\n",
    "        return np.argmin(diff_sqr_sum)\n",
    "    \n",
    "    def activation_metric(self, n_i, n_j):\n",
    "        (x_i, y_i) = self.get_neuron_position(n_i)\n",
    "        (x_j, y_j) = self.get_neuron_position(n_j)\n",
    "        dist_sqr = float(pow(x_i - x_j, 2) + pow(y_i - y_j, 2))\n",
    "        return min(self.var / (1.0 + dist_sqr), 1.0)\n",
    "#         return exp(-dist_sqr / self.var)\n",
    "        #return exp(-dist_sqr / (2.0 * self.var * self.var))\n",
    "    \n",
    "    def adjust_neuron_weight(self, n_best, n_update, input_feature_vec):\n",
    "        if np.shape(input_feature_vec) != (1, self.size_feature_vec):\n",
    "            raise Exception(\"input feature vector {} is but should be {}\".format(np.shape(input_feature_vec), (1, self.size_feature_vec)))\n",
    "        \n",
    "        act_value = self.activation_metric(n_best, n_update) \n",
    "        #d_w = self.learning_rate * act_value * \\\n",
    "        #    (input_feature_vec[0, :] - self.weights[n_update, :])\n",
    "        \n",
    "        d_w = act_value * self.learning_rate * \\\n",
    "            (input_feature_vec[0, :] - self.weights[n_update, :])\n",
    "            \n",
    "#         d_w = 1.0 * (input_feature_vec[0, :] - self.weights[n_update, :])\n",
    "            \n",
    "        self.weights[n_update, :] += d_w\n",
    "        \n",
    "        # record global data\n",
    "        self.avg_activation_val += act_value\n",
    "        self.avg_dw_mean_update += d_w\n",
    "        \n",
    "        if n_update == n_best:\n",
    "            self.avg_dw_max_update = d_w\n",
    "        \n",
    "    def update_hyper_params(self):\n",
    "        # for now, the hyper-params are linearly updated\n",
    "        if self.time_step < 1000:\n",
    "            self.learning_rate = max(self.learning_rate_0*(1.0 - float(self.time_step) / 8000.0), 0.1)\n",
    "        else:\n",
    "            self.learning_rate = max(0.01*(1.0 - float(self.time_step) / 40000.0), 0.0001)\n",
    "#         self.var = max(self.var_0 * pow(0.99, float(time_step)), 10.0)\n",
    "        self.var = max(self.var_0*(1.0 - float(self.time_step) / 40000.0), 1)\n",
    "        \n",
    "        #TEMP\n",
    "#         self.learning_rate = 1.0\n",
    "#         self.var = 1.0\n",
    "        \n",
    "#         print(\"hyperparams: lr={}, var={}\".format(self.learning_rate, self.var))\n",
    "        \n",
    "    def train_single_input(self, input_feature_vec):\n",
    "        # get the best neuron\n",
    "        n_best = self.get_best_neuron(input_feature_vec)\n",
    "        \n",
    "        # update all neurons\n",
    "#         for n in range(0, self.dim_neuron * self.dim_neuron):\n",
    "#             self.adjust_neuron_weight(n_best, n, input_feature_vec)\n",
    "        \n",
    "        #TEST: adjust my neuron and the 8 neurons around me\n",
    "        n_x, n_y = self.get_neuron_position(n_best)\n",
    "        for n in range(0, self.dim_neuron * self.dim_neuron):\n",
    "            x_pos, y_pos = self.get_neuron_position(n)\n",
    "            if pow(n_x - x_pos, 2) + pow(n_y - y_pos, 2) <= self.var * self.var:\n",
    "                self.adjust_neuron_weight(n_best, n, input_feature_vec)\n",
    "                \n",
    "#         for x in range(0, self.var):\n",
    "#             for y in range(0, 3):\n",
    "#                 x_pos = n_x + x - 1\n",
    "#                 y_pos = n_y + y - 1\n",
    "#                 if x_pos >= 0 and x_pos < self.dim_neuron and y_pos >= 0 and y_pos < self.dim_neuron:\n",
    "#                     idx = self.get_neuron_index(x_pos, y_pos)\n",
    "#                     self.adjust_neuron_weight(n_best, idx, input_feature_vec)\n",
    "        \n",
    "        # return the best neuron used, for debugging\n",
    "        return n_best\n",
    "        \n",
    "    def train_dataset(self, data_train, data_validation, epochs):\n",
    "        # check that the input data is the right dimensionality\n",
    "        if np.shape(data_train)[1] != self.size_feature_vec:\n",
    "            raise Exception(\"input dataset {} is not correct dimension\".format(np.shape(input_data)))\n",
    "        \n",
    "        # reset the stored error values over time\n",
    "        self.error_over_time = []\n",
    "        \n",
    "        num_train = np.shape(data_train)[0]\n",
    "        print(\"training on {} vectors.\".format(num_train))\n",
    "        for e in range(0, epochs):\n",
    "            print(\"\\n\\nNEW EPOCH: {}\".format(e))\n",
    "            # update the Kohonen Map for each training vector\n",
    "            for t in range(start_num, num_train):\n",
    "                self.avg_activation_val = 0.0\n",
    "                self.avg_dw_mean_update = np.zeros(self.size_feature_vec)\n",
    "                self.avg_dw_max_update = np.zeros(self.size_feature_vec)\n",
    "                train_vec = np.asarray([data_train[t, :]])\n",
    "                #print(\"next train vector: {}\".format(train_vec))\n",
    "                t_n_best = self.train_single_input(train_vec)\n",
    "                    \n",
    "                if t % self.steps_until_param_write == 0:\n",
    "                    print(\"time_step: {}\".format(self.time_step))\n",
    "                    print(\"\\titter: {}\".format(t))\n",
    "                    \n",
    "                    # print the global info for this timestep\n",
    "                    self.avg_activation_val /= float(self.dim_neuron * self.dim_neuron)\n",
    "                    self.avg_dw_mean_update /= float(self.dim_neuron * self.dim_neuron)\n",
    "                    print(\"\\tavg activation val: {}\".format(self.avg_activation_val))\n",
    "                    print(\"\\tavg dw mean update: {}\".format(self.avg_dw_mean_update))\n",
    "                    print(\"\\tavg dw max update: {}\".format(self.avg_dw_max_update))\n",
    "                    print(\"\\tsum max update: {}\".format(np.sum(abs(self.avg_dw_max_update))))\n",
    "                    print(\"\\tbest neuron used: {}\".format(t_n_best))\n",
    "                    \n",
    "                    print(\"hyperparams: lr={}, var={}\".format(self.learning_rate, self.var))\n",
    "                    \n",
    "                # print the quantization error\n",
    "                if (t + 1) % self.steps_until_error_test == 0:\n",
    "                    avg_q_error, best_neurons = self.average_quantization_error(data_train)\n",
    "                    self.error_over_time.append(avg_q_error)\n",
    "                    print(\"avg. quantization error at t={}: {}\".format(self.time_step, avg_q_error))\n",
    "                    print('best neurons: {}'.format(best_neurons))\n",
    "                    \n",
    "                # update the hyper params at each timestep\n",
    "                self.update_hyper_params()\n",
    "                    \n",
    "                self.time_step += 1\n",
    "                    \n",
    "                            \n",
    "    # average distance of all input vectors to their best neuron. A measure of how\n",
    "    #  well the map has fit the data\n",
    "    def average_quantization_error(self, input_data):\n",
    "        # check that the input data is the right dimensionality\n",
    "        if np.shape(input_data)[1] != self.size_feature_vec:\n",
    "            raise Exception(\"input dataset {} is not correct dimension\".format(np.shape(input_data)))\n",
    "                      \n",
    "        print(\"START: avg. quantixation error calc\")\n",
    "        num_train = np.shape(input_data)[0]\n",
    "        avg_q_error = 0.0\n",
    "\n",
    "        neuron_choices = {}\n",
    "        for t in range(0, count):         \n",
    "            train_vec = np.asarray([input_data[t, :]])\n",
    "            n_best = self.get_best_neuron(train_vec)\n",
    "            \n",
    "            if n_best in neuron_choices:\n",
    "                neuron_choices[n_best] += 1\n",
    "            else:\n",
    "                neuron_choices[n_best] = 1\n",
    "            \n",
    "            dist_to_best = sum(abs(train_vec[0, :] - self.weights[n_best, :]))\n",
    "            avg_q_error += dist_to_best\n",
    "                            \n",
    "        return avg_q_error / float(count), neuron_choices\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** DATASET: ** we now must compile the dataset which is |U|x|N| where |U| is the number of user_ids we are training on (that must have some minimum number of ads messaged in the top x% of ads - NOTE: could change to views later - NOTE2: views could be 1/10 of a message value) and |N| is the number of ads in the top x% of ads. A single user feature vector is a binary vector where each feature is 1 if the user sent a message to the ad_id associated with that feature, otherwise it is 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ad_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>seller_id</th>\n",
       "      <th>creation_time</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>source</th>\n",
       "      <th>...</th>\n",
       "      <th>user_id</th>\n",
       "      <th>event</th>\n",
       "      <th>channel</th>\n",
       "      <th>user_lat</th>\n",
       "      <th>user_long</th>\n",
       "      <th>origin</th>\n",
       "      <th>images_count</th>\n",
       "      <th>ad_impressions</th>\n",
       "      <th>ad_views</th>\n",
       "      <th>ad_messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1918128</th>\n",
       "      <td>2804693</td>\n",
       "      <td>806</td>\n",
       "      <td>434981</td>\n",
       "      <td>2017-06-07 19:54:42</td>\n",
       "      <td>planta de limon</td>\n",
       "      <td>planta de limón</td>\n",
       "      <td>149.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>apple</td>\n",
       "      <td>...</td>\n",
       "      <td>1966</td>\n",
       "      <td>view</td>\n",
       "      <td>android</td>\n",
       "      <td>-34.543266</td>\n",
       "      <td>-58.692127</td>\n",
       "      <td>browse_search</td>\n",
       "      <td>1.0</td>\n",
       "      <td>693295.0</td>\n",
       "      <td>7165.0</td>\n",
       "      <td>189.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337225</th>\n",
       "      <td>2814436</td>\n",
       "      <td>800</td>\n",
       "      <td>635875</td>\n",
       "      <td>2017-06-08 22:30:32</td>\n",
       "      <td>iPhone 6 en excelente estado !</td>\n",
       "      <td>iPhone 6 en excelente estado !! muy poco uso.c...</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>apple</td>\n",
       "      <td>...</td>\n",
       "      <td>4171</td>\n",
       "      <td>view</td>\n",
       "      <td>android</td>\n",
       "      <td>-34.581360</td>\n",
       "      <td>-58.420368</td>\n",
       "      <td>search</td>\n",
       "      <td>6.0</td>\n",
       "      <td>205481.0</td>\n",
       "      <td>3696.0</td>\n",
       "      <td>118.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1513127</th>\n",
       "      <td>2726025</td>\n",
       "      <td>815</td>\n",
       "      <td>490708</td>\n",
       "      <td>2017-05-28 22:41:53</td>\n",
       "      <td>Buzo abrigado abercrombie</td>\n",
       "      <td>Muy abrigado, Azul, peludito adentro, letras p...</td>\n",
       "      <td>260.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>apple</td>\n",
       "      <td>...</td>\n",
       "      <td>5751</td>\n",
       "      <td>view</td>\n",
       "      <td>android</td>\n",
       "      <td>-34.663368</td>\n",
       "      <td>-58.508793</td>\n",
       "      <td>notification_center</td>\n",
       "      <td>2.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>730.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417601</th>\n",
       "      <td>2347424</td>\n",
       "      <td>815</td>\n",
       "      <td>540480</td>\n",
       "      <td>2017-04-13 21:02:43</td>\n",
       "      <td>Campera cuero negra. Sin uso</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>android</td>\n",
       "      <td>...</td>\n",
       "      <td>10204</td>\n",
       "      <td>view</td>\n",
       "      <td>android</td>\n",
       "      <td>-34.645706</td>\n",
       "      <td>-58.570988</td>\n",
       "      <td>search</td>\n",
       "      <td>3.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>1122.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307236</th>\n",
       "      <td>2598159</td>\n",
       "      <td>362</td>\n",
       "      <td>335997</td>\n",
       "      <td>2017-05-14 10:47:19</td>\n",
       "      <td>Fiat 147 modelo 87</td>\n",
       "      <td>Papeles al dia</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>android</td>\n",
       "      <td>...</td>\n",
       "      <td>4622</td>\n",
       "      <td>view</td>\n",
       "      <td>android</td>\n",
       "      <td>-34.726551</td>\n",
       "      <td>-58.322262</td>\n",
       "      <td>browse_search</td>\n",
       "      <td>4.0</td>\n",
       "      <td>645.0</td>\n",
       "      <td>527.0</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ad_id  category_id  seller_id        creation_time  \\\n",
       "1918128  2804693          806     434981  2017-06-07 19:54:42   \n",
       "1337225  2814436          800     635875  2017-06-08 22:30:32   \n",
       "1513127  2726025          815     490708  2017-05-28 22:41:53   \n",
       "417601   2347424          815     540480  2017-04-13 21:02:43   \n",
       "1307236  2598159          362     335997  2017-05-14 10:47:19   \n",
       "\n",
       "                                  title  \\\n",
       "1918128                 planta de limon   \n",
       "1337225  iPhone 6 en excelente estado !   \n",
       "1513127       Buzo abrigado abercrombie   \n",
       "417601     Campera cuero negra. Sin uso   \n",
       "1307236              Fiat 147 modelo 87   \n",
       "\n",
       "                                               description    price  lat  \\\n",
       "1918128                                    planta de limón    149.0  NaN   \n",
       "1337225  iPhone 6 en excelente estado !! muy poco uso.c...   8000.0  NaN   \n",
       "1513127  Muy abrigado, Azul, peludito adentro, letras p...    260.0  NaN   \n",
       "417601                                                 NaN    500.0  NaN   \n",
       "1307236                                     Papeles al dia  12000.0  NaN   \n",
       "\n",
       "         long   source     ...       user_id event  channel   user_lat  \\\n",
       "1918128   NaN    apple     ...          1966  view  android -34.543266   \n",
       "1337225   NaN    apple     ...          4171  view  android -34.581360   \n",
       "1513127   NaN    apple     ...          5751  view  android -34.663368   \n",
       "417601    NaN  android     ...         10204  view  android -34.645706   \n",
       "1307236   NaN  android     ...          4622  view  android -34.726551   \n",
       "\n",
       "         user_long               origin  images_count ad_impressions  \\\n",
       "1918128 -58.692127        browse_search           1.0       693295.0   \n",
       "1337225 -58.420368               search           6.0       205481.0   \n",
       "1513127 -58.508793  notification_center           2.0           79.0   \n",
       "417601  -58.570988               search           3.0          257.0   \n",
       "1307236 -58.322262        browse_search           4.0          645.0   \n",
       "\n",
       "         ad_views  ad_messages  \n",
       "1918128    7165.0        189.0  \n",
       "1337225    3696.0        118.0  \n",
       "1513127     730.0         99.0  \n",
       "417601     1122.0         99.0  \n",
       "1307236     527.0         92.0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the data we have to use to create the feature-vector training format:\n",
    "user_dt_ad_dt_twenty_percent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top ad_ids in descending order: [2804693 2814436 2726025 ..., 2665438 2546267 2665525]\n",
      "number of ads: 89407\n"
     ]
    }
   ],
   "source": [
    "# NOTE: the above data has removed duplicate events where ad_ids were the same. This is not good for what we want\n",
    "#  because two events with the same ad_id could be from different users\n",
    "top_ad_ids = user_dt_ad_dt_twenty_percent['ad_id'].values\n",
    "print(\"top ad_ids in descending order: {}\".format(top_ad_ids))\n",
    "print(\"number of ads: {}\".format(len(top_ad_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user ids for training: [    1     3     4 ..., 15056 15061 15066]\n",
      "number of users: 10348\n"
     ]
    }
   ],
   "source": [
    "# get the user_ids used for the training set:\n",
    "user_ids_train = user_ad_interests_dt['user_id'].values\n",
    "print(\"user ids for training: {}\".format(user_ids_train))\n",
    "print(\"number of users: {}\".format(len(user_ids_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user ids for training: [    2     6    14 ..., 15059 15060 15064]\n",
      "number of users: 10507\n"
     ]
    }
   ],
   "source": [
    "# get the user_ids used for the test set:\n",
    "user_messages_test = pd.read_csv('Data/user_messages_test.csv', sep=',', header=0)\n",
    "user_ids_test = user_messages_test['user_id'].values\n",
    "print(\"user ids for training: {}\".format(user_ids_test))\n",
    "print(\"number of users: {}\".format(len(user_ids_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we now iterate through each interaction event of the users and find all the ads they have messaged or with\n",
    "user_data_fm = user_dt[user_dt['event'].apply((lambda event: event == 'first_message'))]\n",
    "user_data_fm_ad = user_data_fm[user_data_fm['ad_id'].apply(lambda ad_id: ad_id in top_ad_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Compiling train data:**  pair users with the ads the have clicked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_ad_data_train = np.zeros(shape=(len(user_ids_train), len(top_ad_ids)))\n",
    "user_ad_data_train_cum = np.zeros(shape=(len(user_ids_train), 1))\n",
    "\n",
    "for u in range(0, len(user_ids_train)):\n",
    "    # create the user feature vector by first extracting all ads of that user (this includes the post-7-day ads)\n",
    "    user_ad_fm = user_data_fm_ad[user_data_fm_ad['user_id'].apply(lambda user_id: user_id == user_ids_train[u])]\n",
    "    user_ads_clicked = user_ad_interests_dt[user_ad_interests_dt['user_id'] == user_ids_train[u]]\n",
    "    # now get each ad id and ad it to the binary feature vector\n",
    "    #user_vec = np.zeros(len(top_ad_ids))\n",
    "    for a in user_ad_fm['ad_id']:\n",
    "        user_ad_data_train[u, np.where(top_ad_ids == a)[0]] = 1\n",
    "    for a in user_ads_clicked['ads']:\n",
    "        ads_usr_click = a.split(',')\n",
    "        ads_usr_click[0] = ads_usr_click[0][1:len(ads_usr_click[0])]\n",
    "        ads_usr_click[len(ads_usr_click) - 1] = ads_usr_click[len(ads_usr_click) - 1][0:len(ads_usr_click[len(ads_usr_click) - 1]) - 1]\n",
    "        for c in ads_usr_click:\n",
    "            user_ad_data_train[u, np.where(top_ad_ids == int(c))[0]] = 1\n",
    "            \n",
    "    user_ad_data_train_cum[u] = np.sum(user_ad_data_train[u, :])\n",
    "            \n",
    "# print(user_ad_data_train)\n",
    "# print(np.sum(user_ad_data_train, axis=1)[0:20])\n",
    "# print(len(user_ad_data_train[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-1f57609fc316>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#clean_data_train_sum = clean_data_train.sum(axis=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#print(clean_data_train_sum.h[ead())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mclean_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_data_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser_ad_data_train_cum_bool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0muser_ad_data_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/levi/virtualenvs/iaml_env/local/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1989\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1990\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1991\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1992\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1993\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/levi/virtualenvs/iaml_env/local/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2031\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2032\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2033\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2034\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2035\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/levi/virtualenvs/iaml_env/local/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, convert, is_copy, **kwargs)\u001b[0m\n\u001b[1;32m   1631\u001b[0m         new_data = self._data.take(indices,\n\u001b[1;32m   1632\u001b[0m                                    \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1633\u001b[0;31m                                    convert=True, verify=True)\n\u001b[0m\u001b[1;32m   1634\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/levi/virtualenvs/iaml_env/local/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[1;32m   3709\u001b[0m         \u001b[0mnew_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3710\u001b[0m         return self.reindex_indexer(new_axis=new_labels, indexer=indexer,\n\u001b[0;32m-> 3711\u001b[0;31m                                     axis=axis, allow_dups=True)\n\u001b[0m\u001b[1;32m   3712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3713\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlsuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/levi/virtualenvs/iaml_env/local/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mreindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy)\u001b[0m\n\u001b[1;32m   3595\u001b[0m             new_blocks = [blk.take_nd(indexer, axis=axis, fill_tuple=(\n\u001b[1;32m   3596\u001b[0m                 fill_value if fill_value is not None else blk.fill_value,))\n\u001b[0;32m-> 3597\u001b[0;31m                 for blk in self.blocks]\n\u001b[0m\u001b[1;32m   3598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3599\u001b[0m         \u001b[0mnew_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/levi/virtualenvs/iaml_env/local/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_tuple)\u001b[0m\n\u001b[1;32m    994\u001b[0m             \u001b[0mfill_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfill_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m             new_values = algos.take_nd(values, indexer, axis=axis,\n\u001b[0;32m--> 996\u001b[0;31m                                        allow_fill=True, fill_value=fill_value)\n\u001b[0m\u001b[1;32m    997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/levi/virtualenvs/iaml_env/local/lib/python2.7/site-packages/pandas/core/algorithms.pyc\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, out, fill_value, mask_info, allow_fill)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'F'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m     func = _get_take_nd_function(arr.ndim, arr.dtype, out.dtype, axis=axis,\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# clean data by removing all users that didn't click on any ads\n",
    "# print(np.shape(np.array([user_ids_train]).T))\n",
    "# print(np.shape(user_ad_data_train))\n",
    "# merge = np.hstack((np.array([user_ids_train]).T, user_ad_data_train))\n",
    "# print(np.shape(merge))\n",
    "clean_data_train = pd.DataFrame(data=user_ad_data_train, columns=range(0, len(top_ad_ids)))\n",
    "# print(np.shape(clean_data_train))\n",
    "user_ad_data_train_cum_bool = (user_ad_data_train_cum > 0)\n",
    "#print(user_ad_data_train_cum_bool)\n",
    "clean_data_ids = pd.DataFrame(data=user_ids_train, columns=['user_id'])\n",
    "# clean_idx = clean_data_train[clean_data_train.apply(lambda row: np.sum(row) > 0, axis=1)]\n",
    "#clean_data_train_sum = clean_data_train.sum(axis=1)\n",
    "#print(clean_data_train_sum.h[ead())\n",
    "clean_idx = clean_data_train[user_ad_data_train_cum_bool]\n",
    "\n",
    "del user_ad_data_train\n",
    "\n",
    "# clean_data_train = clean_data_train[clean_idx]\n",
    "# clean_data_ids = clean_data_ids[clean_idx]\n",
    "\n",
    "# clean_data_train.head()\n",
    "# clean_data_ids.head()\n",
    "\n",
    "# clean_data_train = pd.concat([clean_data_train, clean_data_ids], axis=0)\n",
    "# clean_data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    859\n",
      "1    800\n",
      "2    815\n",
      "3    815\n",
      "4    362\n",
      "Name: category_id, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/levi/virtualenvs/iaml_env/lib/python2.7/site-packages/ipykernel/__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# implement one-hot encoding of the categories\n",
    "user_id_categories = user_ad_interests_dt['category_id']\n",
    "print(user_id_categories.head())\n",
    "\n",
    "for c in categories:\n",
    "    clean_idx['cat_' + str(c)] = (user_id_categories == c).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (1, 10348), indices imply (89417, 8662)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-1476873cf8e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# normalise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclean_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_ad_data_train_cum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/levi/virtualenvs/iaml_env/local/lib/python2.7/site-packages/pandas/core/ops.pyc\u001b[0m in \u001b[0;36mf\u001b[0;34m(self, other, axis, level, fill_value)\u001b[0m\n\u001b[1;32m   1086\u001b[0m                 \u001b[0;31m#                            columns=self.columns)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m                 casted = pd.DataFrame(other, index=self.index,\n\u001b[0;32m-> 1088\u001b[0;31m                                       columns=self.columns)\n\u001b[0m\u001b[1;32m   1089\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/levi/virtualenvs/iaml_env/local/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 mgr = self._init_ndarray(data, index, columns, dtype=dtype,\n\u001b[0;32m--> 255\u001b[0;31m                                          copy=copy)\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/levi/virtualenvs/iaml_env/local/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_init_ndarray\u001b[0;34m(self, values, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_possibly_infer_to_datetimelike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/levi/virtualenvs/iaml_env/local/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[0;34m(blocks, axes)\u001b[0m\n\u001b[1;32m   3991\u001b[0m         \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'values'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3992\u001b[0m         \u001b[0mtot_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3993\u001b[0;31m         \u001b[0mconstruction_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/levi/virtualenvs/iaml_env/local/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mconstruction_error\u001b[0;34m(tot_items, block_shape, axes, e)\u001b[0m\n\u001b[1;32m   3968\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Empty data passed with indices specified.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3969\u001b[0m     raise ValueError(\"Shape of passed values is {0}, indices imply {1}\".format(\n\u001b[0;32m-> 3970\u001b[0;31m         passed, implied))\n\u001b[0m\u001b[1;32m   3971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (1, 10348), indices imply (89417, 8662)"
     ]
    }
   ],
   "source": [
    "# normalise\n",
    "clean_idx = clean_idx.div(user_ad_data_train_cum, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Compiling test data:**  pair users with the ads the have clicked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-866968fbe5e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0muser_ad_data_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_ids_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_ad_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mu\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_ids_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# create the user feature vector by first extracting all ads of that user (this includes the post-7-day ads)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0muser_ad_fm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_data_fm_ad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser_data_fm_ad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0muser_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0muser_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0muser_ids_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "user_ad_data_test = np.zeros(shape=(len(user_ids_test), len(top_ad_ids)))\n",
    "\n",
    "for u in range(0, len(user_ids_test)):\n",
    "    # create the user feature vector by first extracting all ads of that user (this includes the post-7-day ads)\n",
    "    user_ad_fm = user_data_fm_ad[user_data_fm_ad['user_id'].apply(lambda user_id: user_id == user_ids_test[u])]\n",
    "    user_ads_clicked = user_ad_interests_dt[user_ad_interests_dt['user_id'] == user_ids_test[u]]\n",
    "    # now get each ad id and ad it to the binary feature vector\n",
    "    #user_vec = np.zeros(len(top_ad_ids))\n",
    "    for a in user_ad_fm['ad_id']:\n",
    "        user_ad_data_test[u, np.where(top_ad_ids == a)[0]] = 1\n",
    "    for a in user_ads_clicked['ads']:\n",
    "        ads_usr_click = a.split(',')\n",
    "        ads_usr_click[0] = ads_usr_click[0][1:len(ads_usr_click[0])]\n",
    "        ads_usr_click[len(ads_usr_click) - 1] = ads_usr_click[len(ads_usr_click) - 1][0:len(ads_usr_click[len(ads_usr_click) - 1]) - 1]\n",
    "        for c in ads_usr_click:\n",
    "            user_ad_data_test[u, np.where(top_ad_ids == int(c))[0]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** We now train the Kohonan Map using the user data: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting weights: \n",
      "[[ 0.  1.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  1.  0.]\n",
      " ..., \n",
      " [ 0.  0.  1. ...,  0.  1.  0.]\n",
      " [ 0.  1.  0. ...,  1.  0.  0.]\n",
      " [ 1.  1.  0. ...,  0.  0.  0.]]\n",
      "starting weights: \n",
      "[[  1.26376284e-21   2.56702857e-13   1.01771764e-15 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  8.34347023e-30   5.39453599e-05   1.46502692e-94 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  1.93786091e-18   1.71967235e-03   5.61486195e-03 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " ..., \n",
      " [  1.89193385e-34   7.78060452e-03   2.01462131e-14 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  7.78709363e-03   5.27259916e-04   2.81148739e-05 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  8.20929765e-02   5.52132670e-03   1.44432219e-03 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]]\n",
      "training on 8662 vectors.\n",
      "\n",
      "\n",
      "NEW EPOCH: 0\n",
      "time_step: 27100\n",
      "\titter: 800\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -6.04047816e-06  -8.98267330e-07  -9.48512479e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.30499829e-05  -1.56070193e-06  -8.32206977e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0426180482923\n",
      "\tbest neuron used: 23\n",
      "hyperparams: lr=0.00322525, var=1\n",
      "time_step: 27300\n",
      "\titter: 1000\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -7.19343305e-07  -1.28968189e-07  -8.13018838e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -4.49420617e-09  -6.01000062e-09  -5.55357439e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0189040500496\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.00317525, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [23, 18, 18, 18, 18, 23, 13, 23, 23, 23, 23, 23, 18, 18, 18, 18, 14, 14, 23, 23, 23, 18, 18, 18, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 18, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 13, 18, 18, 23, 23, 23, 23, 23, 23, 13, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 18, 18, 18, 18, 13, 23, 23, 23, 23, 13, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 9, 9, 9, 9, 23, 23, 13, 13, 13, 13, 13, 13, 13, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 3, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 18, 18, 23, 23, 23, 23, 23, 13, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 13, 13, 23, 23, 23, 23, 13, 13, 13, 23, 23, 23, 23, 23, 23, 23, 23, 19, 19, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 13, 13, 18, 18, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 18, 18, 13, 13, 13, 23, 23, 23, 23, 23, 23, 18, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 18, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 13, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24, 24, 24, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 4, 23, 23, 18, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 14, 14, 23, 23, 23, 18, 18, 18, 23, 23, 22, 23, 23, 13, 23, 23, 13, 13, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 13, 13, 13, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 14, 23, 18, 18, 14, 14, 14, 23, 23, 23, 23, 14, 14, 23, 23, 23, 23, 13, 13, 13, 13, 23, 23, 23, 23, 18, 24, 18, 18, 18, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 13, 13, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 18, 23, 23, 23, 23, 23, 23, 23, 18, 23, 23, 23, 23, 23, 23, 23, 13, 23, 23, 23, 13, 23, 23, 13, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 13, 23, 23, 23, 18, 23, 23, 23, 22, 23, 23, 13, 13, 13, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 14, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 14, 23, 23, 23, 23, 23, 23, 23, 13, 23, 23, 23, 23, 23, 23, 9, 23, 18, 18, 18, 18, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 18, 18, 18, 18, 23, 23, 18, 23, 23, 23, 18, 18, 13, 18, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 19, 23, 23, 23, 23, 23, 23, 18, 18, 23, 23, 23, 23, 18, 18, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23]\n",
      "avg. quantization error at t=27499: 13.0440796532\n",
      "time_step: 27500\n",
      "\titter: 1200\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -4.69855555e-06  -7.32684738e-07  -7.53311165e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.44296251e-05  -9.77022147e-07  -5.20973692e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0269389134353\n",
      "\tbest neuron used: 23\n",
      "hyperparams: lr=0.00312525, var=1\n",
      "time_step: 27700\n",
      "\titter: 1400\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -3.55901446e-06  -5.68328189e-07  -3.67846502e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -8.74990820e-06  -5.92451572e-07  -3.15910631e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0416165416129\n",
      "\tbest neuron used: 23\n",
      "hyperparams: lr=0.00307525, var=1\n",
      "time_step: 27900\n",
      "\titter: 1600\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -6.34277245e-07  -7.23991115e-08  -1.07808399e-06 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.39198547e-09  -1.86147080e-09  -1.04012108e-05 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.039391227523\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.00302525, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [23, 18, 18, 18, 18, 23, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 23, 23, 23, 23, 23, 19, 19, 19, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 13, 23, 23, 23, 23, 23, 23, 23, 23, 23, 18, 18, 18, 23, 23, 23, 23, 23, 23, 23, 23, 13, 18, 18, 23, 23, 23, 23, 23, 23, 18, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 23, 23, 23, 23, 13, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 9, 9, 9, 9, 23, 23, 13, 13, 13, 13, 13, 13, 13, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 19, 19, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24, 24, 23, 23, 23, 3, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 18, 18, 23, 23, 23, 23, 23, 18, 23, 23, 18, 18, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 13, 13, 18, 18, 18, 23, 23, 23, 23, 23, 23, 23, 23, 19, 19, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 13, 13, 18, 18, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 13, 23, 23, 23, 23, 23, 23, 23, 23, 14, 14, 14, 14, 14, 23, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 23, 23, 23, 23, 19, 23, 18, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 18, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 13, 23, 23, 23, 23, 23, 23, 23, 24, 24, 24, 23, 23, 23, 18, 18, 24, 24, 24, 18, 18, 18, 18, 23, 23, 23, 23, 23, 23, 23, 23, 23, 4, 23, 23, 18, 23, 23, 23, 23, 18, 18, 18, 18, 23, 23, 23, 23, 23, 14, 14, 23, 23, 23, 18, 18, 18, 23, 23, 22, 23, 23, 24, 23, 23, 13, 13, 18, 18, 18, 18, 23, 23, 23, 23, 23, 23, 23, 23, 18, 13, 13, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 22, 22, 22, 23, 18, 18, 18, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 13, 13, 14, 14, 14, 23, 23, 23, 23, 23, 23, 23, 18, 23, 23, 13, 13, 13, 13, 23, 23, 23, 23, 18, 24, 18, 18, 18, 23, 23, 23, 18, 23, 23, 23, 23, 23, 23, 13, 13, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 18, 18, 18, 18, 23, 23, 23, 23, 23, 23, 23, 18, 23, 23, 23, 23, 23, 23, 23, 18, 23, 13, 23, 18, 23, 23, 18, 23, 23, 18, 23, 23, 24, 18, 23, 23, 18, 23, 23, 13, 23, 23, 23, 18, 23, 23, 23, 22, 23, 23, 23, 23, 23, 23, 23, 23, 23, 18, 18, 23, 23, 23, 23, 23, 23, 23, 23, 23, 18, 23, 23, 23, 18, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 18, 23, 18, 18, 18, 23, 23, 9, 18, 18, 18, 18, 18, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 23, 23, 18, 23, 23, 23, 18, 18, 18, 23, 18, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 19, 23, 23, 23, 23, 23, 23, 18, 18, 23, 23, 23, 23, 18, 18, 23, 23, 23, 23, 23, 23, 23, 3, 23, 23, 23, 23, 23, 23, 23, 23, 23]\n",
      "avg. quantization error at t=28099: 13.4222683868\n",
      "time_step: 28100\n",
      "\titter: 1800\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -2.77877865e-06  -3.88494670e-07  -1.71894665e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -8.41764874e-06  -2.37509831e-07  -1.26646437e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0352236354634\n",
      "\tbest neuron used: 23\n",
      "hyperparams: lr=0.00297525, var=1\n",
      "time_step: 28300\n",
      "\titter: 2000\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -5.16873774e-07  -3.21349995e-07  -6.13666971e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -5.45886689e-10  -7.30001967e-10  -4.07898118e-06 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0288861319268\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.00292525, var=1\n",
      "time_step: 28500\n",
      "\titter: 2200\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -1.47230255e-07  -1.41007284e-06  -1.33400798e-10 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.44519241e-37  -2.55800778e-05  -2.60375740e-17 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.152699972163\n",
      "\tbest neuron used: 22\n",
      "hyperparams: lr=0.00287525, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 13, 13, 13, 13, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 13, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 23, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 13, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 9, 9, 9, 9, 18, 18, 13, 13, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 23, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 24, 24, 18, 18, 18, 3, 13, 13, 23, 23, 23, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 13, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 23, 23, 18, 18, 23, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 19, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 13, 13, 18, 18, 18, 18, 18, 14, 14, 18, 18, 18, 13, 13, 13, 18, 19, 17, 18, 18, 13, 18, 18, 13, 13, 13, 19, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 23, 24, 24, 24, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 13, 13, 14, 14, 14, 18, 18, 18, 18, 13, 13, 18, 23, 18, 18, 19, 19, 19, 19, 13, 13, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 19, 19, 23, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 13, 18, 18, 13, 18, 18, 18, 18, 18, 24, 19, 18, 18, 18, 18, 18, 13, 18, 18, 18, 13, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 19, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 19, 19, 23, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 9, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 13, 13, 13, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 14, 18, 18, 23, 23, 18, 18, 13, 13, 18, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 3, 18, 18, 18, 18, 18, 18, 18, 18, 18]\n",
      "avg. quantization error at t=28699: 13.1510366305\n",
      "time_step: 28700\n",
      "\titter: 2400\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -7.49681900e-10  -2.60992802e-06  -7.56412722e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.17800406e-08  -4.95298445e-05  -5.42435522e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0355203125901\n",
      "\tbest neuron used: 9\n",
      "hyperparams: lr=0.00282525, var=1\n",
      "time_step: 28900\n",
      "\titter: 2600\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -2.52408434e-06  -1.02578605e-07  -3.26540491e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.23776036e-05  -9.13512680e-08  -1.93224211e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0276421602648\n",
      "\tbest neuron used: 19\n",
      "hyperparams: lr=0.00277525, var=1\n",
      "time_step: 29100\n",
      "\titter: 2800\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -2.00717702e-07  -9.51216449e-07  -1.14501653e-06 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -3.02114726e-09  -8.03225272e-08  -1.87763488e-05 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0314899813663\n",
      "\tbest neuron used: 14\n",
      "hyperparams: lr=0.00272525, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 13, 13, 13, 13, 18, 13, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 18, 18, 18, 18, 18, 18, 18, 13, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 24, 18, 18, 18, 18, 14, 18, 18, 18, 18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 9, 9, 9, 9, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 14, 14, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 24, 24, 18, 18, 18, 3, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 13, 13, 13, 13, 18, 18, 18, 13, 19, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 19, 14, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 14, 14, 14, 18, 18, 18, 18, 23, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 14, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 24, 24, 24, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 13, 13, 13, 18, 18, 13, 13, 18, 18, 18, 18, 18, 14, 14, 18, 18, 13, 13, 13, 13, 18, 18, 17, 18, 18, 14, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 14, 14, 18, 18, 18, 13, 13, 13, 18, 24, 24, 24, 14, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 14, 18, 13, 13, 14, 14, 14, 13, 18, 18, 18, 14, 14, 18, 18, 18, 18, 14, 14, 14, 14, 13, 13, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 19, 18, 18, 18, 18, 18, 19, 19, 18, 13, 13, 13, 19, 18, 18, 18, 19, 19, 19, 18, 13, 18, 18, 18, 18, 18, 18, 18, 13, 18, 19, 18, 13, 18, 18, 13, 18, 13, 18, 18, 18, 24, 19, 18, 18, 18, 18, 18, 13, 18, 18, 18, 13, 18, 18, 18, 17, 18, 18, 14, 14, 14, 18, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 13, 18, 18, 18, 14, 18, 18, 18, 14, 18, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 14, 23, 23, 23, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 9, 23, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 13, 24, 13, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 23, 23, 19, 18, 13, 13, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 3, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=29299: 13.7114591257\n",
      "time_step: 29300\n",
      "\titter: 3000\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -2.09937610e-06  -8.74170973e-08  -4.18363097e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -8.85040985e-06  -6.53192803e-08  -1.38161918e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0283048716204\n",
      "\tbest neuron used: 19\n",
      "hyperparams: lr=0.00267525, var=1\n",
      "time_step: 29500\n",
      "\titter: 3200\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -1.23822815e-07  -1.66382586e-06  -7.39703528e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.02799441e-09  -1.00019615e-05  -1.26039306e-05 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0354209893516\n",
      "\tbest neuron used: 14\n",
      "hyperparams: lr=0.00262525, var=1\n",
      "time_step: 29700\n",
      "\titter: 3400\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -1.32582146e-10  -3.79889034e-06  -7.46773287e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.53124201e-17  -6.59692209e-05  -4.97553765e-06 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0741111401138\n",
      "\tbest neuron used: 3\n",
      "hyperparams: lr=0.00257525, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 18, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 14, 19, 19, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 19, 18, 18, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 9, 9, 9, 9, 18, 18, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 24, 24, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 24, 24, 18, 18, 18, 3, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 22, 22, 22, 18, 18, 18, 18, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 19, 19, 19, 19, 18, 18, 18, 18, 18, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 24, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 14, 18, 18, 18, 18, 18, 18, 18, 23, 18, 24, 24, 24, 18, 18, 9, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 14, 14, 14, 14, 18]\n",
      "avg. quantization error at t=29899: 13.2207685633\n",
      "time_step: 29900\n",
      "\titter: 3600\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -3.80109023e-07  -8.61512603e-08  -6.15959886e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.87974525e-11  -3.85101842e-11  -1.82192382e-06 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0192060270427\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.00252525, var=1\n",
      "time_step: 30100\n",
      "\titter: 3800\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -1.45075777e-06  -1.80079481e-07  -8.26043884e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -5.80048923e-06  -1.72157365e-08  -6.93743362e-06 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0200142072529\n",
      "\tbest neuron used: 19\n",
      "hyperparams: lr=0.00247525, var=1\n",
      "time_step: 30300\n",
      "\titter: 4000\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -1.28520702e-06  -2.21003359e-07  -6.99319692e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -4.36153360e-06  -1.29449448e-08  -5.21643065e-06 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0339756756494\n",
      "\tbest neuron used: 19\n",
      "hyperparams: lr=0.00242525, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 14, 14, 14, 14, 14, 18, 14, 14, 14, 18, 18, 18, 18, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 19, 18, 19, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 14, 18, 18, 18, 18, 19, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 9, 9, 9, 9, 18, 18, 19, 19, 19, 19, 19, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 23, 24, 24, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 19, 19, 19, 18, 18, 18, 19, 18, 18, 18, 24, 24, 18, 18, 18, 3, 13, 13, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 19, 13, 13, 14, 14, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 19, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 19, 19, 18, 18, 18, 13, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 18, 18, 18, 19, 19, 19, 18, 19, 17, 18, 18, 24, 18, 18, 19, 19, 18, 19, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 19, 19, 19, 19, 18, 18, 18, 18, 18, 19, 18, 13, 13, 23, 23, 23, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 18, 24, 18, 18, 18, 18, 18, 18, 19, 23, 23, 18, 18, 18, 18, 19, 19, 19, 18, 23, 23, 23, 23, 23, 18, 18, 18, 19, 18, 18, 18, 18, 18, 19, 19, 18, 19, 19, 19, 19, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 19, 18, 19, 18, 18, 18, 18, 13, 18, 18, 18, 24, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 19, 18, 18, 23, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 24, 24, 24, 18, 18, 19, 23, 18, 18, 18, 18, 19, 19, 18, 18, 19, 18, 19, 19, 19, 19, 18, 19, 19, 19, 19, 23, 23, 19, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 13, 18, 18, 23, 23, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=30499: 13.0370225755\n",
      "time_step: 30500\n",
      "\titter: 4200\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -3.01829074e-07  -5.33675892e-08  -3.07957806e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -9.65570146e-12  -1.29123520e-11  -6.10885720e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0340494195223\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.00237525, var=1\n",
      "time_step: 30700\n",
      "\titter: 4400\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -1.37505587e-06  -2.06264451e-07  -4.39798913e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -8.07681240e-06  -6.10518185e-09  -2.46020807e-06 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.017070189567\n",
      "\tbest neuron used: 19\n",
      "hyperparams: lr=0.00232525, var=1\n",
      "time_step: 30900\n",
      "\titter: 4600\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -1.24360066e-06  -2.98205267e-07  -3.95961964e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -6.19935557e-06  -4.68603099e-09  -1.88833217e-06 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0157830767855\n",
      "\tbest neuron used: 19\n",
      "hyperparams: lr=0.00227525, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 14, 14, 14, 14, 14, 18, 14, 14, 14, 18, 18, 18, 18, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 13, 19, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 9, 9, 9, 9, 18, 18, 19, 19, 19, 19, 19, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 23, 24, 24, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 19, 19, 19, 23, 23, 23, 18, 18, 18, 18, 24, 24, 18, 18, 18, 18, 13, 13, 19, 19, 19, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 13, 13, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 19, 13, 13, 14, 14, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 13, 18, 19, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 19, 8, 19, 19, 14, 18, 18, 19, 19, 18, 19, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 23, 23, 23, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 18, 24, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 19, 19, 19, 18, 23, 23, 23, 23, 23, 18, 18, 18, 19, 18, 18, 18, 18, 18, 19, 19, 18, 19, 19, 19, 19, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 19, 18, 19, 18, 19, 18, 19, 18, 18, 19, 18, 13, 23, 18, 18, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 19, 18, 18, 18, 18, 18, 18, 18, 23, 23, 18, 18, 18, 19, 18, 18, 18, 18, 18, 19, 18, 18, 18, 19, 18, 18, 23, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 23, 18, 24, 24, 24, 18, 18, 14, 19, 18, 18, 18, 18, 19, 19, 18, 19, 19, 18, 19, 19, 19, 19, 18, 19, 19, 19, 19, 23, 23, 18, 18, 18, 18, 18, 18, 19, 24, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 19, 18, 23, 23, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 19, 19, 18, 18, 19, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=31099: 12.4181303965\n",
      "time_step: 31100\n",
      "\titter: 4800\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.78022844e-07  -3.23724079e-08  -2.59889217e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -3.17069088e-12  -4.24009346e-12  -2.00599593e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0234634959837\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.00222525, var=1\n",
      "time_step: 31300\n",
      "\titter: 5000\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.23676182e-07  -2.68084923e-08  -2.08141085e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.18110296e-12  -2.91673983e-12  -1.37991492e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.016507945824\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.00217525, var=1\n",
      "time_step: 31500\n",
      "\titter: 5200\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -9.02068360e-10  -8.92087324e-07  -1.01563144e-06 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.55165433e-13  -5.86900783e-08  -5.40064291e-06 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0152340820755\n",
      "\tbest neuron used: 13\n",
      "hyperparams: lr=0.00212525, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 13, 13, 13, 13, 18, 19, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 14, 14, 14, 18, 14, 14, 14, 18, 18, 18, 18, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 14, 14, 13, 13, 18, 18, 13, 13, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 13, 19, 18, 18, 19, 18, 18, 18, 18, 19, 19, 18, 18, 13, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 13, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 19, 19, 18, 18, 18, 9, 9, 9, 9, 13, 18, 19, 19, 19, 19, 19, 19, 19, 14, 14, 14, 18, 18, 18, 18, 18, 23, 19, 19, 13, 13, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 23, 23, 18, 18, 18, 3, 18, 18, 19, 19, 19, 13, 13, 13, 17, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 13, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 14, 14, 13, 13, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 13, 13, 18, 19, 19, 14, 14, 19, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 13, 23, 13, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 24, 24, 24, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 13, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 19, 8, 18, 18, 14, 18, 18, 19, 19, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 19, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 19, 22, 22, 22, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 19, 19, 19, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 13, 24, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 13, 13, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 13, 18, 13, 13, 18, 18, 18, 18, 18, 13, 23, 18, 18, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 8, 18, 19, 18, 18, 18, 18, 13, 13, 18, 23, 23, 18, 18, 18, 19, 18, 13, 18, 18, 18, 19, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 19, 19, 19, 19, 19, 23, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 23, 18, 13, 13, 13, 19, 19, 19, 13, 18, 18, 18, 18, 18, 18, 18, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 13, 18, 18, 18, 13, 13, 18, 24, 19, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 23, 13, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=31699: 12.54964758\n",
      "time_step: 31700\n",
      "\titter: 5400\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.33780774e-06  -2.10169772e-08  -1.33651275e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.25538822e-12  -1.67880238e-12  -7.94244459e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0161979291137\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.00207525, var=1\n",
      "time_step: 31900\n",
      "\titter: 5600\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.11652495e-06  -1.78076581e-08  -1.07235258e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -8.89614433e-13  -1.18966133e-12  -5.62830941e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0157551238523\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.00202525, var=1\n",
      "time_step: 32100\n",
      "\titter: 5800\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -7.07088762e-10  -7.45027883e-07  -7.12100642e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.26402593e-13  -2.90736013e-08  -2.67534383e-06 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0121546609394\n",
      "\tbest neuron used: 13\n",
      "hyperparams: lr=0.00197525, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 13, 13, 13, 13, 18, 19, 18, 18, 18, 18, 18, 13, 13, 13, 13, 23, 23, 17, 17, 17, 13, 13, 13, 18, 17, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 14, 14, 14, 14, 14, 18, 14, 14, 14, 18, 18, 18, 18, 19, 18, 18, 14, 18, 18, 18, 18, 18, 23, 13, 14, 14, 13, 17, 18, 18, 17, 17, 18, 18, 17, 17, 17, 18, 18, 18, 18, 18, 18, 13, 19, 18, 18, 19, 18, 18, 18, 18, 19, 19, 17, 17, 18, 18, 18, 18, 17, 17, 17, 17, 17, 18, 18, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 19, 18, 17, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 19, 18, 18, 18, 18, 19, 19, 18, 18, 18, 9, 9, 9, 9, 17, 18, 19, 19, 19, 19, 19, 19, 19, 14, 14, 14, 18, 18, 18, 18, 18, 23, 19, 19, 19, 19, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 13, 13, 13, 17, 17, 17, 18, 13, 13, 13, 23, 23, 18, 18, 18, 23, 18, 18, 19, 19, 19, 17, 17, 17, 17, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 17, 17, 18, 18, 18, 18, 18, 18, 17, 17, 18, 18, 14, 14, 13, 13, 13, 17, 17, 18, 18, 18, 18, 18, 19, 14, 14, 14, 14, 14, 14, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 17, 13, 18, 19, 19, 14, 14, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 23, 17, 23, 17, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 23, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 13, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 24, 24, 24, 17, 17, 17, 17, 18, 18, 18, 23, 18, 18, 18, 18, 18, 17, 13, 17, 13, 18, 18, 18, 18, 18, 18, 19, 19, 18, 19, 17, 17, 18, 23, 23, 13, 18, 23, 17, 17, 17, 17, 19, 8, 18, 18, 14, 18, 18, 19, 19, 18, 13, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 14, 14, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 17, 17, 17, 13, 13, 13, 19, 22, 22, 22, 18, 17, 17, 17, 13, 13, 13, 13, 18, 18, 17, 17, 17, 18, 18, 23, 23, 13, 13, 13, 18, 19, 19, 19, 13, 13, 18, 19, 17, 17, 17, 17, 17, 17, 19, 19, 18, 18, 13, 24, 17, 17, 17, 18, 18, 18, 17, 23, 23, 17, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 23, 23, 23, 18, 17, 17, 18, 18, 18, 18, 18, 18, 19, 19, 19, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 17, 18, 17, 13, 17, 18, 18, 18, 18, 13, 23, 13, 18, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 8, 18, 19, 18, 18, 18, 18, 17, 17, 18, 13, 13, 18, 18, 18, 19, 18, 18, 17, 18, 18, 19, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 19, 17, 19, 19, 19, 19, 19, 19, 18, 18, 17, 13, 13, 13, 13, 13, 13, 13, 23, 18, 23, 23, 23, 19, 19, 19, 14, 18, 18, 18, 18, 19, 19, 18, 14, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 19, 19, 18, 18, 13, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=32299: 12.6061623127\n",
      "time_step: 32300\n",
      "\titter: 6000\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -2.50699920e-06  -2.76123839e-07  -5.08069256e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.20468423e-05  -2.25371858e-07  -2.98806040e-10 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0296737793081\n",
      "\tbest neuron used: 23\n",
      "hyperparams: lr=0.00192525, var=1\n",
      "time_step: 32500\n",
      "\titter: 6200\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -6.02052718e-10  -6.54972895e-07  -6.00471358e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -9.38509633e-14  -2.15864677e-08  -1.98638010e-06 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0285079517687\n",
      "\tbest neuron used: 13\n",
      "hyperparams: lr=0.00187525, var=1\n",
      "time_step: 32700\n",
      "\titter: 6400\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -5.81055824e-11  -2.51086329e-06  -3.95220642e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.47797607e-17  -3.85190074e-05  -2.90518471e-06 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0718848123979\n",
      "\tbest neuron used: 3\n",
      "hyperparams: lr=0.00182525, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 23, 13, 18, 18, 13, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 19, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 14, 14, 14, 18, 18, 18, 18, 18, 23, 19, 19, 18, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 18, 18, 18, 23, 19, 19, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 19, 14, 14, 14, 14, 14, 14, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 13, 18, 19, 19, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 17, 23, 19, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 23, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 18, 18, 18, 13, 18, 13, 18, 19, 19, 18, 18, 18, 19, 19, 18, 19, 18, 18, 18, 23, 23, 13, 18, 23, 18, 18, 18, 18, 19, 8, 18, 18, 19, 18, 18, 19, 19, 18, 13, 19, 19, 19, 19, 19, 17, 17, 17, 17, 17, 19, 14, 14, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 13, 13, 13, 19, 22, 22, 22, 18, 17, 17, 17, 13, 13, 13, 13, 18, 18, 18, 18, 18, 14, 18, 23, 23, 13, 13, 13, 13, 19, 19, 19, 13, 13, 18, 19, 18, 18, 17, 17, 17, 17, 23, 23, 19, 18, 13, 24, 18, 18, 18, 17, 17, 18, 18, 23, 23, 18, 18, 18, 18, 23, 23, 18, 18, 23, 23, 23, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 23, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 19, 18, 19, 13, 18, 18, 18, 18, 18, 13, 23, 13, 18, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 8, 18, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 19, 18, 18, 18, 18, 18, 19, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 19, 19, 19, 19, 19, 19, 18, 18, 18, 14, 14, 19, 19, 19, 19, 19, 13, 18, 18, 18, 18, 19, 19, 19, 13, 18, 18, 18, 18, 18, 18, 18, 19, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 14, 23, 18, 23, 23, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 23, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=32899: 12.7988409642\n",
      "time_step: 32900\n",
      "\titter: 6600\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -5.85502657e-07  -8.44630252e-09  -4.69673319e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.30786436e-13  -3.08625500e-13  -1.46011286e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0195738950012\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.00177525, var=1\n",
      "time_step: 33100\n",
      "\titter: 6800\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -2.22508786e-07  -1.24547255e-06  -8.80220465e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.93801674e-10  -7.06477060e-06  -1.35095149e-05 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0643125465047\n",
      "\tbest neuron used: 14\n",
      "hyperparams: lr=0.00172525, var=1\n",
      "time_step: 33300\n",
      "\titter: 7000\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -4.22484275e-07  -6.12240435e-09  -1.28255095e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.27747769e-13  -1.70834213e-13  -8.08219768e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00981751253732\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.00167525, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 19, 13, 13, 13, 13, 13, 18, 19, 19, 19, 18, 18, 18, 18, 17, 13, 13, 19, 18, 18, 18, 18, 18, 23, 13, 13, 13, 13, 17, 18, 18, 18, 18, 13, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 19, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 23, 23, 13, 13, 13, 13, 13, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 13, 18, 18, 19, 18, 18, 18, 18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 14, 14, 14, 18, 18, 18, 18, 18, 23, 19, 19, 18, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 19, 19, 19, 17, 17, 17, 18, 13, 13, 13, 23, 23, 18, 18, 18, 23, 17, 17, 19, 19, 19, 19, 19, 19, 17, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 13, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 17, 19, 19, 18, 18, 18, 18, 13, 13, 13, 18, 18, 18, 23, 18, 18, 23, 18, 18, 18, 19, 18, 18, 18, 18, 13, 18, 18, 18, 18, 19, 18, 18, 18, 13, 13, 18, 18, 23, 23, 23, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 23, 18, 18, 18, 18, 18, 8, 18, 18, 13, 18, 18, 19, 19, 18, 13, 13, 13, 18, 18, 18, 17, 17, 17, 17, 17, 17, 18, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 17, 17, 17, 13, 13, 13, 13, 18, 18, 18, 18, 18, 14, 18, 23, 23, 13, 13, 13, 13, 13, 13, 13, 13, 13, 18, 18, 18, 18, 13, 13, 13, 13, 23, 23, 13, 18, 13, 24, 18, 18, 18, 17, 17, 18, 18, 23, 23, 18, 18, 18, 18, 23, 23, 18, 18, 23, 23, 23, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 23, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 19, 18, 19, 13, 18, 18, 18, 18, 18, 13, 23, 13, 18, 24, 23, 13, 18, 18, 18, 18, 18, 18, 18, 17, 13, 18, 18, 18, 8, 18, 18, 23, 23, 23, 23, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 23, 23, 23, 23, 23, 19, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 13, 18, 13, 13, 13, 19, 19, 19, 13, 13, 13, 13, 13, 18, 18, 18, 13, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 19, 18, 18, 18, 23, 23, 19, 19, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 14, 23, 18, 23, 23, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 19, 19, 18, 18, 23, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=33499: 12.620613828\n",
      "time_step: 33500\n",
      "\titter: 7200\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -8.62403703e-07  -5.30893240e-09  -1.10132787e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -9.90778453e-14  -1.32494570e-13  -6.26834219e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.011263662559\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.00162525, var=1\n",
      "time_step: 33700\n",
      "\titter: 7400\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -7.42880783e-07  -4.61434398e-09  -9.47217215e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -7.69265188e-14  -1.02872100e-13  -4.86689776e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.011096093261\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.00157525, var=1\n",
      "time_step: 33900\n",
      "\titter: 7600\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -6.33085420e-07  -3.90487598e-09  -7.99499262e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -5.70783408e-14  -7.63295788e-14  -3.61116626e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00881103851386\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.00152525, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 23, 13, 18, 18, 18, 17, 18, 18, 18, 18, 13, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 13, 13, 13, 13, 13, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 23, 23, 23, 23, 23, 14, 14, 14, 18, 18, 18, 18, 18, 23, 19, 19, 18, 18, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 17, 17, 17, 18, 18, 18, 18, 23, 23, 18, 18, 18, 19, 17, 17, 19, 19, 19, 19, 19, 19, 17, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 13, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 23, 18, 23, 19, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 23, 23, 23, 13, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 17, 19, 19, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 23, 18, 18, 18, 19, 18, 18, 18, 18, 13, 18, 18, 18, 18, 19, 18, 18, 18, 18, 13, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 19, 19, 18, 18, 18, 18, 18, 23, 18, 18, 18, 18, 13, 13, 18, 18, 23, 18, 18, 18, 18, 18, 8, 18, 18, 19, 18, 18, 19, 19, 18, 13, 13, 13, 18, 18, 18, 17, 17, 17, 17, 17, 17, 18, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 19, 22, 22, 22, 23, 17, 17, 17, 13, 13, 13, 13, 18, 18, 18, 18, 18, 23, 18, 23, 23, 13, 13, 13, 13, 19, 19, 19, 13, 13, 18, 18, 18, 18, 17, 17, 17, 17, 19, 19, 19, 18, 13, 24, 18, 18, 18, 17, 17, 18, 13, 13, 13, 18, 18, 18, 18, 23, 23, 18, 18, 23, 23, 23, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 23, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 19, 18, 19, 13, 17, 18, 18, 18, 18, 13, 23, 13, 18, 24, 19, 13, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 8, 18, 18, 23, 23, 23, 19, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 23, 23, 23, 23, 23, 23, 18, 18, 19, 14, 14, 18, 18, 18, 18, 18, 13, 18, 23, 23, 23, 17, 17, 23, 13, 19, 19, 19, 19, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 13, 13, 19, 18, 18, 18, 23, 23, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 18, 23, 23, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 19, 19, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18]\n",
      "avg. quantization error at t=34099: 12.3694158868\n",
      "time_step: 34100\n",
      "\titter: 7800\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -5.86033969e-07  -3.36458478e-09  -6.81191783e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -4.36457871e-14  -5.83665274e-14  -2.76133103e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00831143265491\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.00147525, var=1\n",
      "time_step: 34300\n",
      "\titter: 8000\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -3.62786887e-10  -6.60221845e-07  -4.41166406e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.09490229e-14  -4.81844182e-09  -1.48632168e-06 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0171045886751\n",
      "\tbest neuron used: 13\n",
      "hyperparams: lr=0.00142525, var=1\n",
      "time_step: 34500\n",
      "\titter: 8200\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -4.29643998e-07  -3.88476872e-08  -4.94077015e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.52511465e-14  -3.37677891e-14  -1.59756024e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.007432801259\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.00137525, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 14, 14, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 19, 19, 19, 19, 19, 19, 18, 19, 19, 19, 18, 18, 18, 18, 17, 13, 13, 19, 18, 18, 18, 18, 18, 23, 13, 18, 18, 18, 17, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 13, 13, 13, 13, 13, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 17, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 23, 23, 23, 23, 23, 13, 13, 13, 18, 18, 18, 18, 18, 23, 19, 19, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 19, 19, 19, 17, 17, 17, 18, 18, 18, 18, 23, 23, 18, 18, 18, 19, 17, 17, 19, 19, 19, 19, 19, 19, 17, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 19, 18, 23, 23, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 14, 14, 13, 13, 13, 19, 19, 17, 18, 18, 19, 19, 18, 14, 14, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 18, 14, 14, 18, 23, 17, 17, 18, 18, 19, 18, 18, 18, 18, 18, 23, 19, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 23, 23, 23, 19, 18, 18, 18, 17, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 17, 19, 19, 13, 13, 18, 18, 13, 13, 13, 18, 18, 18, 23, 18, 18, 23, 18, 18, 18, 19, 18, 18, 18, 18, 13, 18, 18, 18, 18, 19, 18, 18, 18, 18, 13, 17, 17, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 19, 19, 18, 18, 18, 18, 18, 23, 19, 18, 18, 18, 23, 23, 18, 18, 23, 18, 18, 18, 18, 18, 8, 18, 18, 18, 18, 18, 19, 19, 18, 13, 13, 13, 18, 18, 18, 17, 17, 17, 17, 17, 17, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 13, 13, 13, 19, 22, 22, 22, 23, 17, 17, 17, 13, 13, 13, 13, 18, 18, 18, 18, 18, 23, 18, 23, 23, 13, 13, 13, 13, 19, 19, 19, 13, 13, 18, 19, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 13, 24, 13, 13, 13, 17, 17, 18, 13, 13, 13, 18, 18, 18, 18, 23, 23, 18, 18, 23, 23, 23, 23, 23, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 23, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 19, 18, 19, 13, 17, 18, 18, 18, 18, 13, 23, 13, 18, 24, 19, 13, 18, 18, 18, 18, 18, 18, 18, 17, 13, 18, 18, 18, 8, 18, 18, 23, 23, 23, 19, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 23, 18, 18, 17, 18, 18, 18, 19, 18, 18, 19, 18, 18, 18, 18, 18, 19, 18, 23, 23, 23, 23, 23, 23, 18, 18, 23, 13, 13, 19, 19, 19, 19, 19, 13, 18, 23, 23, 23, 17, 17, 19, 13, 19, 19, 19, 19, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 13, 13, 13, 18, 18, 18, 23, 23, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 18, 23, 23, 19, 18, 18, 18, 18, 17, 17, 18, 13, 13, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 23, 23, 23, 23, 23, 23, 18]\n",
      "avg. quantization error at t=34699: 11.9872904876\n",
      "time_step: 34700\n",
      "\titter: 8400\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -3.72112491e-07  -3.34296858e-08  -4.24905966e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.95102567e-14  -2.60906265e-14  -1.23435229e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00687677090478\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.00132525, var=1\n",
      "time_step: 34900\n",
      "\titter: 8600\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -3.06908886e-10  -6.53156508e-07  -3.56627058e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.31014239e-14  -6.69639051e-07  -9.29538831e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0110749597956\n",
      "\tbest neuron used: 13\n",
      "hyperparams: lr=0.00127525, var=1\n",
      "\n",
      "\n",
      "NEW EPOCH: 1\n",
      "time_step: 35062\n",
      "\titter: 800\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -1.45414522e-06  -1.24461006e-07  -2.04319859e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -7.56852594e-06  -2.94280756e-08  -3.90167912e-11 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0159131458603\n",
      "\tbest neuron used: 23\n",
      "hyperparams: lr=0.00123475, var=1\n",
      "time_step: 35262\n",
      "\titter: 1000\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.55269472e-07  -2.25530889e-08  -2.85897284e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.05609940e-14  -1.41229793e-14  -6.68160720e-10 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00612754954763\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.00118475, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 18, 18, 13, 13, 13, 13, 23, 23, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 23, 13, 18, 18, 18, 17, 18, 18, 18, 18, 13, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 13, 13, 13, 13, 13, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 17, 23, 23, 19, 19, 19, 19, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 23, 23, 19, 19, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 19, 19, 19, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 17, 17, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 19, 19, 17, 18, 18, 23, 23, 23, 14, 14, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 13, 19, 19, 14, 14, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 23, 17, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 17, 18, 17, 17, 18, 18, 18, 18, 18, 18, 18, 17, 19, 19, 18, 18, 18, 18, 13, 13, 13, 18, 18, 18, 23, 18, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 17, 18, 18, 18, 18, 13, 17, 17, 23, 23, 23, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 19, 19, 23, 18, 18, 18, 18, 23, 19, 23, 23, 18, 13, 13, 18, 18, 23, 23, 23, 23, 18, 19, 8, 18, 18, 23, 18, 18, 19, 19, 18, 13, 23, 23, 18, 18, 18, 17, 17, 17, 17, 17, 17, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 13, 13, 13, 19, 22, 22, 22, 23, 17, 17, 17, 13, 13, 13, 13, 18, 18, 18, 18, 18, 23, 18, 23, 23, 13, 13, 13, 13, 18, 18, 18, 13, 13, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 19, 19, 13, 24, 13, 13, 13, 17, 17, 18, 13, 13, 13, 18, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 23, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 19, 18, 19, 18, 17, 18, 18, 18, 18, 13, 23, 18, 18, 24, 23, 13, 18, 18, 18, 18, 13, 18, 18, 17, 13, 18, 18, 18, 12, 18, 18, 23, 23, 23, 23, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 23, 18, 18, 19, 18, 18, 18, 17, 18, 18, 19, 23, 18, 18, 18, 18, 19, 18, 23, 23, 23, 23, 23, 23, 18, 18, 23, 13, 13, 19, 19, 19, 19, 19, 13, 18, 18, 18, 18, 17, 17, 14, 13, 19, 19, 19, 19, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 13, 18, 18, 18, 23, 23, 13, 19, 18, 18, 18, 18, 18, 23, 23, 13, 13, 13, 13, 13, 18, 18, 18, 23, 18, 23, 23, 13, 18, 18, 18, 18, 17, 17, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18]\n",
      "avg. quantization error at t=35461: 12.0626344431\n",
      "time_step: 35462\n",
      "\titter: 1200\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.24909783e-07  -1.98364789e-08  -2.51282772e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -8.71878029e-15  -1.16594284e-14  -5.51609678e-10 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00827828446445\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.00113475, var=1\n",
      "time_step: 35662\n",
      "\titter: 1400\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.38910719e-10  -9.59244856e-08  -6.63089465e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -6.14479133e-10  -3.62009771e-08   0.00000000e+00 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0137936509094\n",
      "\tbest neuron used: 17\n",
      "hyperparams: lr=0.00108475, var=1\n",
      "time_step: 35862\n",
      "\titter: 1600\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -1.14381130e-06  -9.41174166e-08  -1.54830804e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -5.63073873e-06  -1.78340506e-08  -2.36450198e-11 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0124703254507\n",
      "\tbest neuron used: 23\n",
      "hyperparams: lr=0.00103475, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 23, 19, 19, 19, 19, 19, 13, 13, 13, 13, 23, 23, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 19, 19, 18, 17, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 13, 13, 13, 13, 13, 18, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 18, 19, 19, 19, 19, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 17, 23, 19, 19, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 19, 19, 19, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 17, 17, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 13, 13, 13, 13, 13, 19, 19, 18, 18, 18, 23, 23, 23, 14, 14, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 19, 19, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 23, 13, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 17, 17, 18, 18, 18, 18, 18, 18, 18, 19, 19, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 23, 23, 23, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 19, 19, 23, 18, 18, 13, 13, 23, 19, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 19, 8, 18, 18, 23, 18, 18, 19, 19, 18, 13, 23, 23, 18, 18, 18, 17, 17, 17, 17, 17, 17, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 22, 22, 22, 23, 13, 13, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 23, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 19, 19, 13, 24, 13, 13, 13, 17, 17, 18, 18, 13, 13, 18, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 23, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 19, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 19, 13, 18, 18, 18, 18, 13, 18, 18, 17, 19, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 23, 18, 18, 19, 18, 18, 18, 19, 18, 18, 19, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 17, 17, 14, 13, 19, 19, 19, 19, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 23, 23, 13, 18, 18, 18, 23, 23, 13, 19, 18, 18, 18, 18, 18, 23, 23, 13, 13, 13, 13, 13, 18, 18, 18, 23, 18, 23, 23, 13, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18]\n",
      "avg. quantization error at t=36061: 12.1543953474\n",
      "time_step: 36062\n",
      "\titter: 1800\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.90713562e-07  -1.37590190e-08  -5.36882394e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -4.87857370e-15  -6.52400666e-15  -3.08651942e-10 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0102580369347\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.00098475, var=1\n",
      "time_step: 36262\n",
      "\titter: 2000\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.85235941e-07  -4.64212545e-08  -4.79993905e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -4.05009548e-15  -5.41610141e-15  -2.56236743e-10 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00784142485246\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.00093475, var=1\n",
      "time_step: 36462\n",
      "\titter: 2200\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -8.40405614e-08  -2.96190205e-07  -3.20705205e-13 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -3.24806545e-38  -5.03552831e-06  -3.45869487e-18 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0491578414486\n",
      "\tbest neuron used: 22\n",
      "hyperparams: lr=0.00088475, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 19, 19, 18, 17, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 19, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 19, 19, 13, 13, 13, 13, 13, 18, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 17, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 23, 23, 23, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 13, 13, 13, 13, 13, 23, 23, 18, 18, 18, 19, 19, 23, 14, 14, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 19, 19, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 17, 17, 18, 18, 18, 18, 18, 18, 18, 19, 19, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 17, 19, 12, 18, 18, 23, 18, 18, 19, 19, 18, 13, 23, 23, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 19, 22, 22, 22, 23, 18, 18, 18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 23, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 13, 13, 18, 23, 18, 18, 19, 19, 19, 19, 19, 19, 19, 18, 13, 24, 13, 13, 13, 18, 18, 18, 18, 13, 13, 23, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 19, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 19, 13, 18, 18, 18, 18, 13, 18, 18, 18, 13, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 19, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 13, 13, 23, 23, 23, 23, 23, 23, 18, 18, 18, 18, 17, 17, 14, 13, 19, 19, 19, 19, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 23, 23, 13, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 19, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18]\n",
      "avg. quantization error at t=36661: 12.194324934\n",
      "time_step: 36662\n",
      "\titter: 2400\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -9.70698879e-11  -1.67592248e-06  -4.36844514e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.91063994e-09  -2.56709770e-05  -5.66189836e-06 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0128611674943\n",
      "\tbest neuron used: 9\n",
      "hyperparams: lr=0.00083475, var=1\n",
      "time_step: 36862\n",
      "\titter: 2600\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.51507281e-07  -3.21445864e-08  -3.32107451e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.37314826e-15  -3.17355768e-15  -1.50141591e-10 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00660598827768\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.00078475, var=1\n",
      "time_step: 37062\n",
      "\titter: 2800\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.33263622e-07  -2.83242842e-08  -7.24280098e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.00737646e-15  -2.68441930e-15  -5.24080739e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00618344508061\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.00073475, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 19, 19, 18, 17, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 17, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 23, 23, 23, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 23, 14, 14, 23, 23, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 13, 18, 18, 19, 19, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 17, 17, 18, 18, 18, 18, 18, 18, 18, 19, 19, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 23, 23, 18, 23, 23, 18, 18, 18, 18, 18, 18, 17, 19, 12, 18, 18, 23, 18, 18, 19, 19, 18, 13, 23, 23, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 13, 13, 13, 23, 22, 22, 22, 23, 18, 18, 18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 23, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 14, 14, 19, 19, 18, 18, 13, 24, 13, 13, 13, 18, 18, 18, 18, 13, 13, 23, 18, 18, 18, 19, 19, 18, 18, 17, 17, 17, 17, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 19, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 24, 19, 18, 18, 18, 18, 18, 13, 18, 18, 18, 19, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 19, 18, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 23, 23, 23, 23, 23, 23, 18, 18, 18, 18, 17, 17, 14, 13, 19, 19, 19, 19, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 23, 18, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=37261: 12.2785265935\n",
      "time_step: 37262\n",
      "\titter: 3000\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.38057043e-10  -2.91804837e-07  -3.16350033e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -3.05360165e-15  -1.56075472e-07  -2.05464141e-06 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00633102335255\n",
      "\tbest neuron used: 13\n",
      "hyperparams: lr=0.00068475, var=1\n",
      "time_step: 37462\n",
      "\titter: 3200\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -6.14880328e-07  -8.56398440e-08  -8.40863955e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.67986958e-06  -4.11222313e-07  -8.61602364e-12 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00736965382006\n",
      "\tbest neuron used: 23\n",
      "hyperparams: lr=0.00063475, var=1\n",
      "time_step: 37662\n",
      "\titter: 3400\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -1.28062335e-11  -1.15460266e-06  -2.11882857e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -4.17031926e-18  -1.63722925e-05  -2.09294685e-06 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0188666128135\n",
      "\tbest neuron used: 3\n",
      "hyperparams: lr=0.00058475, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 23, 23, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 19, 19, 18, 17, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 19, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 23, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 23, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 19, 19, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 17, 19, 12, 18, 18, 23, 18, 18, 19, 19, 18, 13, 13, 13, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 23, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 19, 19, 19, 19, 18, 18, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 18, 13, 13, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 19, 19, 19, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 24, 19, 18, 18, 18, 18, 18, 13, 18, 18, 18, 13, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 19, 18, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 23, 23, 23, 23, 23, 23, 18, 19, 19, 19, 18, 18, 14, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 17, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 23, 18, 18, 23, 23, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=37861: 12.2322056513\n",
      "time_step: 37862\n",
      "\titter: 3600\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -9.27489823e-08  -1.72032550e-08  -5.96587963e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.05868114e-15  -1.41575043e-15  -2.76397778e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00345075448605\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.00053475, var=1\n",
      "time_step: 38062\n",
      "\titter: 3800\n",
      "\tavg activation val: 0.08\n",
      "\tavg dw mean update: [ -8.49141229e-07  -1.19055614e-08  -1.10900602e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.92005857e-05  -1.12386449e-07  -2.93991373e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00562268873805\n",
      "\tbest neuron used: 24\n",
      "hyperparams: lr=0.00048475, var=1\n",
      "time_step: 38262\n",
      "\titter: 4000\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -7.02312022e-08  -2.06702592e-08  -4.46788257e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -7.57694547e-16  -9.48623589e-08  -1.97816965e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00540073840853\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.00043475, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 19, 19, 18, 23, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 19, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 23, 23, 23, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 19, 19, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 19, 19, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 17, 18, 12, 18, 18, 23, 18, 18, 19, 19, 18, 13, 13, 13, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 13, 13, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 19, 19, 19, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 23, 23, 23, 23, 23, 23, 18, 19, 19, 19, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 17, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=38461: 12.2101171328\n",
      "time_step: 38462\n",
      "\titter: 4200\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -6.93247426e-11  -6.02650715e-08  -2.91019206e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.02564138e-10  -3.79752415e-07   0.00000000e+00 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00553191153133\n",
      "\tbest neuron used: 17\n",
      "hyperparams: lr=0.00038475, var=1\n",
      "time_step: 38662\n",
      "\titter: 4400\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -5.80542917e-08  -1.48893171e-08  -3.21013113e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -5.21700136e-16  -6.53161696e-08  -1.36204144e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00212639464078\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.00033475, var=1\n",
      "time_step: 38862\n",
      "\titter: 4600\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -2.69190596e-07  -4.09046834e-08  -4.76508398e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.15244010e-06  -4.57435856e-08  -2.53233258e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00206453160564\n",
      "\tbest neuron used: 19\n",
      "hyperparams: lr=0.00028475, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 19, 19, 18, 23, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 19, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 23, 23, 23, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 19, 19, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 19, 19, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 17, 19, 12, 18, 18, 23, 18, 18, 19, 19, 18, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 13, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 13, 13, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 19, 19, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 23, 23, 23, 23, 23, 23, 18, 19, 19, 19, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 17, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=39061: 12.0822744814\n",
      "time_step: 39062\n",
      "\titter: 4800\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -4.17844428e-11  -3.57859215e-08  -1.73499834e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -5.83657345e-11  -2.16104080e-07   0.00000000e+00 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00264359940479\n",
      "\tbest neuron used: 17\n",
      "hyperparams: lr=0.00023475, var=1\n",
      "time_step: 39262\n",
      "\titter: 5000\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.99915765e-08  -1.06091963e-08  -1.73062846e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.55563435e-16  -6.88391797e-08  -6.67218514e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00133986628609\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.00018475, var=1\n",
      "time_step: 39462\n",
      "\titter: 5200\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.69635293e-08  -7.62956391e-09  -1.24506628e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.82430858e-16  -4.91400133e-08  -4.76285843e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000840399964935\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.00013475, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 19, 19, 18, 23, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 19, 19, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 17, 19, 12, 18, 18, 23, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 19, 19, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 23, 23, 23, 23, 23, 23, 18, 19, 19, 19, 19, 19, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 17, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=39661: 12.0117107007\n",
      "time_step: 39662\n",
      "\titter: 5400\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.02656515e-08  -5.61423679e-09  -9.15801113e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.33664502e-16  -3.60041908e-08  -3.48967881e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000722959335316\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 39862\n",
      "\titter: 5600\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.01056264e-08  -5.56379921e-09  -9.07572129e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.31806154e-16  -3.55036217e-08  -3.44116154e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000722131471574\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 40062\n",
      "\titter: 5800\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.99609022e-08  -5.52027192e-09  -8.99586678e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.30168760e-16  -3.50625694e-08  -3.39841288e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000521732551588\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 23, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 19, 19, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 17, 19, 12, 18, 18, 23, 18, 18, 19, 19, 18, 13, 13, 13, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 19, 19, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 23, 23, 23, 23, 23, 23, 18, 19, 19, 19, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 17, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=40261: 12.011765068\n",
      "time_step: 40262\n",
      "\titter: 6000\n",
      "\tavg activation val: 0.08\n",
      "\tavg dw mean update: [ -1.80277157e-07  -2.49609396e-09  -2.06922948e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -4.01153819e-06  -2.15672494e-08  -5.64177027e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00186380069787\n",
      "\tbest neuron used: 24\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 40462\n",
      "\titter: 6200\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.88625701e-11  -4.69880309e-08  -4.57930200e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -3.12676362e-16  -2.55246682e-08  -2.85419149e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0015082160633\n",
      "\tbest neuron used: 13\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 40662\n",
      "\titter: 6400\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -2.10448910e-12  -1.94989277e-07  -3.52605938e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -6.98436074e-19  -2.74199623e-06  -3.50522224e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00415252075758\n",
      "\tbest neuron used: 3\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 23, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 19, 19, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 19, 12, 18, 18, 23, 18, 18, 19, 19, 18, 18, 13, 13, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 19, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 23, 23, 23, 23, 23, 23, 18, 19, 19, 19, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 17, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=40861: 12.0089267501\n",
      "time_step: 40862\n",
      "\titter: 6600\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.88029282e-11  -4.67259790e-08  -4.54506769e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -3.08591452e-16  -2.51912053e-08  -2.81690337e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00110838573665\n",
      "\tbest neuron used: 13\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 41062\n",
      "\titter: 6800\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -9.32203705e-09  -8.82726933e-08  -4.74030992e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -8.11899252e-12  -6.20652212e-07  -6.27766774e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.003811533044\n",
      "\tbest neuron used: 14\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 41262\n",
      "\titter: 7000\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.93672160e-08  -5.27030738e-09  -8.98782893e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.21265063e-16  -3.26642482e-08  -3.16595742e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000519586181312\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 23, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 19, 19, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 19, 12, 18, 18, 23, 18, 18, 19, 19, 18, 18, 13, 13, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 23, 23, 23, 23, 23, 23, 18, 19, 19, 19, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 17, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=41461: 11.9951930947\n",
      "time_step: 41462\n",
      "\titter: 7200\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.12270325e-08  -5.23288480e-09  -8.92393262e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.19932406e-16  -3.23052805e-08  -3.13116474e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000619304502322\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 41662\n",
      "\titter: 7400\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.10917167e-08  -5.19704972e-09  -8.85840053e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.18673722e-16  -3.19662384e-08  -3.09830335e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000619741981297\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 41862\n",
      "\titter: 7600\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.09276875e-08  -5.14601558e-09  -8.77377809e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.16807489e-16  -3.14635454e-08  -3.04958021e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000518429347909\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 23, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 19, 12, 18, 18, 23, 18, 18, 19, 19, 18, 13, 13, 13, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 23, 23, 23, 23, 23, 23, 18, 19, 19, 19, 18, 18, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 17, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=42061: 11.9763855947\n",
      "time_step: 42062\n",
      "\titter: 7800\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.09720970e-08  -5.09984617e-09  -8.69460213e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.15131685e-16  -3.10121466e-08  -3.00582873e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0005174657223\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 42262\n",
      "\titter: 8000\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.85409794e-11  -4.72726746e-08  -4.50444372e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.92544413e-16  -2.38812395e-08  -2.76564241e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00120710853322\n",
      "\tbest neuron used: 13\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 42462\n",
      "\titter: 8200\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.06489288e-08  -5.80080046e-09  -8.53421689e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.11673029e-16  -4.00142327e-08  -2.91553104e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000515301121646\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 23, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 18, 18, 19, 19, 18, 13, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 23, 23, 23, 23, 23, 23, 18, 19, 19, 19, 18, 18, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=42661: 11.935100795\n",
      "time_step: 42662\n",
      "\titter: 8400\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.05069375e-08  -5.74553168e-09  -8.45066344e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.09982861e-16  -3.94086183e-08  -2.87140455e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000513573248675\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 42862\n",
      "\titter: 8600\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.84282811e-11  -4.75440318e-08  -4.45171306e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.84979037e-16  -2.81553429e-08  -2.69412122e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000904244231233\n",
      "\tbest neuron used: 13\n",
      "hyperparams: lr=0.0001, var=1\n",
      "\n",
      "\n",
      "NEW EPOCH: 2\n",
      "time_step: 43024\n",
      "\titter: 800\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -1.01180364e-07  -1.34789556e-08  -6.68345548e-10 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -5.28206088e-07  -5.75281919e-08  -9.18413371e-13 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00134900201923\n",
      "\tbest neuron used: 23\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 43224\n",
      "\titter: 1000\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.01102996e-08  -5.60811804e-09  -8.24989270e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.05929411e-16  -3.79562022e-08  -2.76557810e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000511914819036\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 19, 19, 18, 23, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 19, 19, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 19, 12, 18, 18, 23, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 18, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 23, 23, 23, 23, 23, 23, 18, 19, 19, 19, 18, 18, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=43423: 11.9279763055\n",
      "time_step: 43424\n",
      "\titter: 1200\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.99797538e-08  -5.56397155e-09  -8.18511603e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.04665802e-16  -3.75034311e-08  -2.73258813e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000712449797482\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 43624\n",
      "\titter: 1400\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.69123309e-11  -1.51902488e-08  -6.89301672e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.11958750e-11  -9.19322564e-08   0.00000000e+00 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00130954421031\n",
      "\tbest neuron used: 17\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 43824\n",
      "\titter: 1600\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -1.00701005e-07  -1.32796902e-08  -6.40014623e-10 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -5.24268231e-07  -5.60152664e-08  -8.94260152e-13 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00124913492468\n",
      "\tbest neuron used: 23\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 19, 19, 18, 23, 18, 18, 18, 18, 18, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 19, 19, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 19, 12, 18, 18, 23, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 23, 18, 19, 19, 19, 18, 18, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=44023: 11.9264225321\n",
      "time_step: 44024\n",
      "\titter: 1800\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.99505685e-08  -5.41917930e-09  -8.38708616e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.00586772e-16  -3.60418493e-08  -2.62609385e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0010136286925\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 44224\n",
      "\titter: 2000\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.99928435e-08  -5.76513348e-09  -8.32550207e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -9.91833479e-17  -3.55389801e-08  -2.58945361e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000813613601982\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 44424\n",
      "\titter: 2200\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -1.03458868e-08  -3.08498175e-08  -1.74792991e-14 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -3.02796541e-39  -5.09827255e-07  -3.22432186e-19 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00560150672755\n",
      "\tbest neuron used: 22\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 19, 19, 18, 23, 18, 18, 18, 18, 18, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 19, 19, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 19, 12, 18, 18, 23, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 23, 23, 23, 23, 23, 23, 18, 19, 19, 19, 18, 18, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=44623: 11.9477751205\n",
      "time_step: 44624\n",
      "\titter: 2400\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -1.05082108e-11  -2.06988472e-07  -5.79717024e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.07803177e-10  -3.12560277e-06  -7.34717810e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00156355224574\n",
      "\tbest neuron used: 9\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 44824\n",
      "\titter: 2600\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.99252148e-08  -5.61166888e-09  -8.11976084e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -9.51275179e-17  -3.40857113e-08  -2.48356504e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000814385697655\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 45024\n",
      "\titter: 2800\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.97619093e-08  -5.56017722e-09  -8.85026522e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -9.38659482e-17  -3.36336707e-08  -3.43872417e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000815499417627\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 19, 19, 18, 23, 18, 18, 18, 18, 18, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 19, 19, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 19, 12, 18, 18, 23, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 24, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 23, 23, 23, 23, 23, 23, 18, 19, 19, 19, 18, 18, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=45223: 11.9627038032\n",
      "time_step: 45224\n",
      "\titter: 3000\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.80506524e-11  -4.65055147e-08  -4.59100928e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.61442117e-16  -2.58299436e-08  -2.85658723e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0009091645815\n",
      "\tbest neuron used: 13\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 45424\n",
      "\titter: 3200\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -9.90310912e-08  -1.36267173e-08  -7.76716854e-10 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -5.07555486e-07  -6.21683179e-08  -8.41044746e-13 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00115516289349\n",
      "\tbest neuron used: 23\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 45624\n",
      "\titter: 3400\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -2.03129528e-12  -1.98645185e-07  -3.63578378e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -6.91417113e-19  -2.78400410e-06  -3.66932249e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00325779009293\n",
      "\tbest neuron used: 3\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 19, 19, 18, 23, 18, 18, 18, 18, 18, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 23, 18, 19, 19, 19, 18, 18, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 17, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=45823: 11.9587693703\n",
      "time_step: 45824\n",
      "\titter: 3600\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.95921486e-08  -5.37524386e-09  -9.17078291e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -8.92387541e-17  -3.19756730e-08  -3.26920962e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000617232294728\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 46024\n",
      "\titter: 3800\n",
      "\tavg activation val: 0.08\n",
      "\tavg dw mean update: [ -1.76601772e-07  -2.49990445e-09  -2.15412661e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -3.92867052e-06  -2.01413248e-08  -5.26876308e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00116637678402\n",
      "\tbest neuron used: 24\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 46224\n",
      "\titter: 4000\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.93067762e-08  -5.68528812e-09  -9.02113510e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -8.69222678e-17  -3.61401417e-08  -3.18434650e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00121699150969\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 19, 19, 18, 23, 18, 18, 18, 18, 18, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 13, 13, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 23, 18, 19, 19, 19, 18, 18, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 17, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=46423: 11.9424482755\n",
      "time_step: 46424\n",
      "\titter: 4200\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.64936002e-11  -1.59355428e-08  -7.04566109e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.91720506e-11  -1.02268811e-07   0.00000000e+00 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00141763365736\n",
      "\tbest neuron used: 17\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 46624\n",
      "\titter: 4400\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.95744332e-08  -5.58084271e-09  -8.86336916e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -8.44207249e-17  -3.51000617e-08  -3.09270394e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000616211360872\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 46824\n",
      "\titter: 4600\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -9.69118435e-08  -1.47991502e-08  -1.68104840e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -4.63260043e-07  -2.79039372e-08  -9.36711351e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000724347586822\n",
      "\tbest neuron used: 19\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 19, 19, 18, 23, 18, 18, 18, 18, 18, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 19, 12, 18, 18, 23, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 13, 13, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 23, 23, 23, 23, 23, 23, 18, 19, 19, 19, 18, 18, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 17, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=47023: 11.9432941269\n",
      "time_step: 47024\n",
      "\titter: 4800\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.64318165e-11  -1.61782453e-08  -6.99897954e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.87464217e-11  -1.04963763e-07   0.00000000e+00 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00111454456631\n",
      "\tbest neuron used: 17\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 47224\n",
      "\titter: 5000\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.90824012e-08  -7.00228557e-09  -8.81814885e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -8.06735551e-17  -5.32183909e-08  -2.95542856e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000713181649217\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 47424\n",
      "\titter: 5200\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.15113094e-08  -6.93834977e-09  -8.74327193e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -7.95917315e-17  -5.25047381e-08  -2.91579659e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000612399662684\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 23, 18, 18, 18, 18, 18, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 19, 12, 18, 18, 23, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 19, 19, 19, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 23, 18, 19, 19, 19, 19, 19, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 17, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=47623: 11.8987338177\n",
      "time_step: 47624\n",
      "\titter: 5400\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.17741205e-08  -6.88431620e-09  -8.67540002e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -7.86894983e-17  -5.19095567e-08  -2.88274380e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000711905039208\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 47824\n",
      "\titter: 5600\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.16011207e-08  -6.81967428e-09  -8.59827498e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -7.75877124e-17  -5.11827352e-08  -2.84238052e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000711218114876\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 48024\n",
      "\titter: 5800\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.14446958e-08  -6.76378313e-09  -8.52342234e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -7.66200291e-17  -5.05443779e-08  -2.80693000e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000510963605597\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 23, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 19, 12, 18, 18, 23, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 13, 13, 19, 19, 19, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 23, 23, 23, 23, 23, 23, 18, 19, 19, 19, 18, 18, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 17, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=48223: 11.9162275422\n",
      "time_step: 48224\n",
      "\titter: 6000\n",
      "\tavg activation val: 0.08\n",
      "\tavg dw mean update: [ -1.80725150e-07  -2.72022737e-09  -2.06196784e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -3.98590776e-06  -1.95245478e-08  -5.10742057e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00186979575303\n",
      "\tbest neuron used: 24\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 48424\n",
      "\titter: 6200\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.76615258e-11  -4.73389949e-08  -4.50796201e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.33098125e-16  -3.25319193e-08  -2.73055111e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00150294811321\n",
      "\tbest neuron used: 13\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 48624\n",
      "\titter: 6400\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -1.99765437e-12  -1.97199481e-07  -3.59396477e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -6.85187757e-19  -2.75892148e-06  -3.63626355e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00417035855185\n",
      "\tbest neuron used: 3\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 23, 18, 18, 18, 18, 18, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 13, 13, 19, 19, 19, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 19, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 23, 23, 23, 23, 23, 23, 18, 19, 19, 19, 18, 18, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 17, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=48823: 11.915965643\n",
      "time_step: 48824\n",
      "\titter: 6600\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.76057012e-11  -4.70699853e-08  -4.47467778e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.30041350e-16  -3.21053060e-08  -2.69474353e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00110305361569\n",
      "\tbest neuron used: 13\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 49024\n",
      "\titter: 6800\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -9.95865606e-09  -8.72911344e-08  -4.73679850e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -7.18684341e-12  -6.00411031e-07  -6.23092287e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00381874207002\n",
      "\tbest neuron used: 14\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 49224\n",
      "\titter: 7000\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.07918634e-08  -6.44675927e-09  -8.53981856e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -7.13684215e-17  -4.70800196e-08  -2.61454043e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000509504857962\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 23, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 13, 13, 19, 19, 19, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 23, 18, 19, 19, 19, 18, 18, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 17, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=49423: 11.9020073257\n",
      "time_step: 49424\n",
      "\titter: 7200\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.26421007e-08  -6.39905538e-09  -8.47950967e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -7.05805802e-17  -4.65602997e-08  -2.58567832e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000609310805186\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 49624\n",
      "\titter: 7400\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.24978372e-08  -6.35363397e-09  -8.41778406e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -6.98398410e-17  -4.60716520e-08  -2.55854177e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000609828374202\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 49824\n",
      "\titter: 7600\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.23206831e-08  -6.28848049e-09  -8.33875369e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -6.87346829e-17  -4.53426060e-08  -2.51805495e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000508684019348\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 23, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 18, 18, 19, 19, 18, 13, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 13, 13, 19, 19, 19, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 23, 18, 19, 19, 19, 18, 18, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 17, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=50023: 11.887951572\n",
      "time_step: 50024\n",
      "\titter: 7800\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.23536780e-08  -6.22985055e-09  -8.26405845e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -6.77451780e-17  -4.46898536e-08  -2.48180502e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000507850595777\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 50224\n",
      "\titter: 8000\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.73638858e-11  -4.75879557e-08  -4.43832882e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.18068086e-16  -3.04342789e-08  -2.64970252e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00120229642109\n",
      "\tbest neuron used: 13\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 50424\n",
      "\titter: 8200\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.20092735e-08  -6.90514869e-09  -8.11342498e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -6.57067685e-17  -5.32788820e-08  -2.40712908e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000505961590954\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 23, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 18, 18, 19, 19, 18, 13, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 13, 13, 19, 19, 19, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 23, 18, 19, 19, 19, 18, 18, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=50623: 11.8526540895\n",
      "time_step: 50624\n",
      "\titter: 8400\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.18579278e-08  -6.83749517e-09  -8.03508404e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -6.47122983e-17  -5.24725075e-08  -2.37069724e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000504350281849\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 50824\n",
      "\titter: 8600\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.72584099e-11  -4.78434875e-08  -4.38737826e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.12418091e-16  -3.45374344e-08  -2.58105055e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000899561104232\n",
      "\tbest neuron used: 13\n",
      "hyperparams: lr=0.0001, var=1\n",
      "\n",
      "\n",
      "NEW EPOCH: 3\n",
      "time_step: 50986\n",
      "\titter: 800\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -1.02182169e-07  -1.37713617e-08  -5.60564870e-10 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -5.65366178e-07  -6.52016007e-08  -6.97246851e-13 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00134255819645\n",
      "\tbest neuron used: 23\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 51186\n",
      "\titter: 1000\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.14316903e-08  -6.66944255e-09  -7.84724185e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -6.23304240e-17  -5.05411448e-08  -2.28343867e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000502759451471\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 23, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 18, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 13, 13, 19, 19, 19, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 23, 18, 19, 19, 19, 18, 18, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=51385: 11.8352297774\n",
      "time_step: 51386\n",
      "\titter: 1200\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.12915821e-08  -6.61523566e-09  -7.78588414e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -6.15868977e-17  -4.99382503e-08  -2.25620002e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000703218341013\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 51586\n",
      "\titter: 1400\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.56775596e-11  -1.56713359e-08  -6.57913921e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.61658064e-11  -9.96452090e-08   0.00000000e+00 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00130124972052\n",
      "\tbest neuron used: 17\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 51786\n",
      "\titter: 1600\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -1.01659739e-07  -1.35549498e-08  -5.37022692e-10 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -5.60340966e-07  -6.34741774e-08  -6.78774291e-13 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00124264067843\n",
      "\tbest neuron used: 23\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 19, 19, 18, 23, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 18, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 13, 13, 19, 19, 19, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 23, 18, 19, 19, 19, 18, 18, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=51985: 11.8488512079\n",
      "time_step: 51986\n",
      "\titter: 1800\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -1.01271788e-07  -1.34879956e-08  -5.31403738e-10 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -5.56181789e-07  -6.30030351e-08  -6.73736033e-13 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00114386958254\n",
      "\tbest neuron used: 23\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 52186\n",
      "\titter: 2000\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.12656276e-08  -6.77475336e-09  -7.94334432e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -5.83667778e-17  -4.73271892e-08  -2.13823281e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000804871399883\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 52386\n",
      "\titter: 2200\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -1.10492773e-08  -3.08260420e-08  -1.32686716e-14 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.81465196e-39  -4.98527662e-07  -2.99717553e-19 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00560479215068\n",
      "\tbest neuron used: 22\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 19, 19, 18, 23, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 13, 13, 19, 19, 19, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 23, 18, 19, 19, 19, 18, 18, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=52585: 11.8717647023\n",
      "time_step: 52586\n",
      "\titter: 2400\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -1.00433349e-11  -2.06652755e-07  -5.88157111e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.99196135e-10  -3.10350313e-06  -7.53376761e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00157686945541\n",
      "\tbest neuron used: 9\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 52786\n",
      "\titter: 2600\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.11714890e-08  -6.59088047e-09  -7.75077024e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -5.59940269e-17  -4.54032243e-08  -2.05130846e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000806047740728\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 52986\n",
      "\titter: 2800\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.09979095e-08  -6.52931867e-09  -8.48487779e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -5.52514409e-17  -4.48010922e-08  -3.01220014e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000807136506318\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 19, 19, 18, 23, 18, 18, 18, 18, 18, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 13, 13, 19, 19, 19, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 23, 18, 19, 19, 19, 18, 18, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=53185: 11.8868537689\n",
      "time_step: 53186\n",
      "\titter: 3000\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.69077276e-11  -4.67679962e-08  -4.53350875e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.94971574e-16  -3.17007742e-08  -2.75408654e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000904564260887\n",
      "\tbest neuron used: 13\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 53386\n",
      "\titter: 3200\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -9.98871952e-08  -1.38613996e-08  -6.83464922e-10 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -5.41403285e-07  -6.91748852e-08  -6.38286201e-13 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00114898743406\n",
      "\tbest neuron used: 23\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 53586\n",
      "\titter: 3400\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -1.92885127e-12  -2.00842909e-07  -3.70295808e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -6.78301936e-19  -2.80075927e-06  -3.79904690e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00327514262802\n",
      "\tbest neuron used: 3\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 19, 19, 18, 23, 18, 18, 18, 18, 18, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 13, 13, 19, 19, 19, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 23, 18, 19, 19, 19, 18, 18, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=53785: 11.8979206483\n",
      "time_step: 53786\n",
      "\titter: 3600\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.07928657e-08  -6.30758994e-09  -8.81915370e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -5.25277786e-17  -4.25925879e-08  -2.86371141e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000609312052579\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 53986\n",
      "\titter: 3800\n",
      "\tavg activation val: 0.08\n",
      "\tavg dw mean update: [ -1.76805659e-07  -2.67149852e-09  -2.14570689e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -3.90395499e-06  -1.82300303e-08  -4.76878814e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00117241180125\n",
      "\tbest neuron used: 24\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 54186\n",
      "\titter: 4000\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.04911295e-08  -6.59954950e-09  -8.67685056e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -5.11668052e-17  -4.64835341e-08  -2.78951382e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00120925710549\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 19, 19, 18, 23, 18, 18, 18, 18, 18, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 13, 13, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 23, 18, 19, 19, 19, 18, 18, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=54385: 11.8782604432\n",
      "time_step: 54386\n",
      "\titter: 4200\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.52969174e-11  -1.63465879e-08  -6.75140512e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.46244564e-11  -1.09258836e-07   0.00000000e+00 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00141002071404\n",
      "\tbest neuron used: 17\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 54586\n",
      "\titter: 4400\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.07388617e-08  -6.47500553e-09  -8.52561551e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -4.96893021e-17  -4.51412661e-08  -2.70896325e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000608583442281\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 54786\n",
      "\titter: 4600\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -9.74508071e-08  -1.48050208e-08  -1.66673407e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -4.88773892e-07  -3.32393209e-08  -9.42341672e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000723124199924\n",
      "\tbest neuron used: 19\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 23, 18, 18, 18, 18, 18, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 19, 12, 18, 18, 23, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 13, 13, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 23, 18, 19, 19, 19, 18, 18, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=54985: 11.8700412252\n",
      "time_step: 54986\n",
      "\titter: 4800\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.52420030e-11  -1.65741446e-08  -6.70881034e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.42983564e-11  -1.11787923e-07   0.00000000e+00 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00110697011033\n",
      "\tbest neuron used: 17\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 55186\n",
      "\titter: 5000\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.02177248e-08  -7.86689073e-09  -8.49157638e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -4.74837507e-17  -6.28138979e-08  -2.58872091e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000705866863531\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 55386\n",
      "\titter: 5200\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.26370566e-08  -7.79416483e-09  -8.41991067e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -4.68469988e-17  -6.19715704e-08  -2.55400645e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000605189019083\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 23, 18, 18, 18, 18, 18, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 19, 12, 18, 18, 23, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 19, 19, 19, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 23, 18, 19, 19, 19, 19, 19, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=55585: 11.8446417215\n",
      "time_step: 55586\n",
      "\titter: 5400\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.28928007e-08  -7.73276328e-09  -8.35494822e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -4.63159522e-17  -6.12690752e-08  -2.52505483e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000704763168902\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 55786\n",
      "\titter: 5600\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.27109599e-08  -7.65926256e-09  -8.28123394e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -4.56674506e-17  -6.04112046e-08  -2.48969979e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000704181069919\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 55986\n",
      "\titter: 5800\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.25466108e-08  -7.59554508e-09  -8.20953599e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -4.50978806e-17  -5.96577487e-08  -2.45864796e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000504028640366\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 23, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 18, 18, 19, 19, 18, 13, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 19, 19, 19, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 23, 18, 19, 19, 19, 18, 18, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 17, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=56185: 11.8566618925\n",
      "time_step: 56186\n",
      "\titter: 6000\n",
      "\tavg activation val: 0.08\n",
      "\tavg dw mean update: [ -1.80876121e-07  -2.87364546e-09  -2.05319139e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -3.96232950e-06  -1.76735492e-08  -4.62321840e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00187554137172\n",
      "\tbest neuron used: 24\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 56386\n",
      "\titter: 6200\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.65428497e-11  -4.75488959e-08  -4.45697813e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.73825230e-16  -3.77648563e-08  -2.63903335e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00149863270069\n",
      "\tbest neuron used: 13\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 56586\n",
      "\titter: 6400\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -1.89690699e-12  -1.99377862e-07  -3.66036333e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -6.72190742e-19  -2.77552570e-06  -3.76481920e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0041878579891\n",
      "\tbest neuron used: 3\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 23, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 18, 18, 19, 19, 18, 13, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 17, 17, 17, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 19, 19, 19, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 17, 17, 17, 17, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 19, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 23, 18, 19, 19, 19, 18, 18, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=56785: 11.8620288091\n",
      "time_step: 56786\n",
      "\titter: 6600\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.64905697e-11  -4.72755343e-08  -4.42435671e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.71545740e-16  -3.72696199e-08  -2.60442590e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00109868624947\n",
      "\tbest neuron used: 13\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 56986\n",
      "\titter: 6800\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -1.04245181e-08  -8.63592104e-08  -4.71107319e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -6.36521520e-12  -5.82798178e-07  -6.14279910e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00382572903648\n",
      "\tbest neuron used: 14\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 57186\n",
      "\titter: 7000\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.18501421e-08  -7.23493367e-09  -8.43702756e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -4.20005294e-17  -5.55604165e-08  -2.77783118e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000503017289865\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 23, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 18, 18, 19, 19, 18, 13, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 17, 17, 17, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 19, 19, 19, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 17, 17, 17, 17, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 19, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 23, 18, 19, 19, 19, 18, 18, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=57385: 11.8505793817\n",
      "time_step: 57386\n",
      "\titter: 7200\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.36933034e-08  -7.18071982e-09  -8.37713719e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -4.15368824e-17  -5.49470809e-08  -2.74716649e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000602884438495\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 57586\n",
      "\titter: 7400\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.35424039e-08  -7.12908592e-09  -8.31589177e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -4.11009552e-17  -5.43704144e-08  -2.71833514e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000603454860969\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 57786\n",
      "\titter: 7600\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.33570605e-08  -7.05510709e-09  -8.23724826e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -4.04505664e-17  -5.35100473e-08  -2.67531972e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000502436672619\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 23, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 18, 18, 19, 19, 18, 13, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 17, 17, 17, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 13, 13, 19, 19, 19, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 17, 17, 17, 17, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 19, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 23, 18, 19, 19, 19, 18, 18, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=57985: 11.8408947893\n",
      "time_step: 57986\n",
      "\titter: 7800\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.33823418e-08  -6.98848314e-09  -8.16298033e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -3.98682397e-17  -5.27397163e-08  -2.63680580e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000501693594273\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 58186\n",
      "\titter: 8000\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.62635394e-11  -4.77740718e-08  -4.42884683e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.62600812e-16  -3.53262660e-08  -2.61132659e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00119832181401\n",
      "\tbest neuron used: 13\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 58386\n",
      "\titter: 8200\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.30221736e-08  -7.64733035e-09  -8.01292234e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -3.86686297e-17  -6.10865295e-08  -2.55746598e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000499990996462\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 23, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 18, 18, 19, 19, 18, 13, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 17, 17, 17, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 13, 13, 19, 19, 19, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 19, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 23, 18, 19, 19, 19, 18, 18, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=58585: 11.8087302398\n",
      "time_step: 58586\n",
      "\titter: 8400\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.28627197e-08  -7.57152746e-09  -7.93459126e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -3.80814767e-17  -6.01589782e-08  -2.51863286e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000498450543784\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 58786\n",
      "\titter: 8600\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.61654192e-11  -4.80176670e-08  -4.37807921e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.58372098e-16  -3.92989879e-08  -2.54341456e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00089573946947\n",
      "\tbest neuron used: 13\n",
      "hyperparams: lr=0.0001, var=1\n",
      "\n",
      "\n",
      "NEW EPOCH: 4\n",
      "time_step: 58948\n",
      "\titter: 800\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -1.02846487e-07  -1.39021687e-08  -5.79983312e-10 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -5.93392525e-07  -7.10062288e-08  -5.29128556e-13 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00133742459193\n",
      "\tbest neuron used: 23\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 59148\n",
      "\titter: 1000\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.24170660e-08  -7.38378725e-09  -7.74802689e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -3.66798067e-17  -5.79446985e-08  -2.42592920e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00049709336018\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 23, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 17, 17, 17, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 13, 13, 19, 19, 19, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 23, 18, 19, 19, 19, 18, 18, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=59347: 11.7942543343\n",
      "time_step: 59348\n",
      "\titter: 1200\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.22705420e-08  -7.32327816e-09  -7.68710403e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -3.62422611e-17  -5.72534886e-08  -2.39699081e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000697621822838\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 59548\n",
      "\titter: 1400\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.45542472e-11  -1.59838769e-08  -6.49735057e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.23300577e-11  -1.05530201e-07   0.00000000e+00 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00129488677014\n",
      "\tbest neuron used: 17\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 59748\n",
      "\titter: 1600\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -1.02298494e-07  -1.36746546e-08  -5.55113183e-10 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -5.87624795e-07  -6.91250202e-08  -5.15110050e-13 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00123763512856\n",
      "\tbest neuron used: 23\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 19, 19, 18, 23, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 18, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 13, 13, 19, 19, 19, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 23, 18, 19, 19, 19, 18, 18, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=59947: 11.8056910914\n",
      "time_step: 59948\n",
      "\titter: 1800\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -1.01905303e-07  -1.36047430e-08  -5.49122076e-10 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -5.83204777e-07  -6.86050731e-08  -5.11235477e-13 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00113877802985\n",
      "\tbest neuron used: 23\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 60148\n",
      "\titter: 2000\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.22148307e-08  -7.45488886e-09  -7.84511535e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -3.43421537e-17  -5.42518056e-08  -2.27132150e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00079954353209\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 60348\n",
      "\titter: 2200\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -1.15826332e-08  -3.07566661e-08  -1.00724521e-14 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.61571194e-39  -4.87905042e-07  -2.78533472e-19 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00560789694282\n",
      "\tbest neuron used: 22\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 19, 19, 18, 23, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 13, 13, 19, 19, 19, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 23, 18, 19, 19, 19, 18, 18, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=60547: 11.8276652321\n",
      "time_step: 60548\n",
      "\titter: 2400\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -9.60128850e-12  -2.06398716e-07  -5.94408720e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.90983785e-10  -3.08292849e-06  -7.66495998e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00158996404335\n",
      "\tbest neuron used: 9\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 60748\n",
      "\titter: 2600\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.21042252e-08  -7.25181430e-09  -7.65473674e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -3.29543001e-17  -5.20593525e-08  -2.17953163e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000800906375567\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 60948\n",
      "\titter: 2800\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.19229448e-08  -7.18367046e-09  -8.38921247e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -3.25172641e-17  -5.13689476e-08  -3.13872282e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000801983463791\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 19, 19, 18, 23, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 19, 19, 19, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 23, 18, 19, 19, 19, 18, 18, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=61147: 11.849777465\n",
      "time_step: 61148\n",
      "\titter: 3000\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.58335992e-11  -4.69107651e-08  -4.52504916e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.45357267e-16  -3.60694437e-08  -2.71941557e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000901001775221\n",
      "\tbest neuron used: 13\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 61348\n",
      "\titter: 3200\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -1.00477259e-07  -1.39577574e-08  -6.99029506e-10 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -5.67143750e-07  -7.45007851e-08  -4.84457005e-13 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00114411511532\n",
      "\tbest neuron used: 23\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 61548\n",
      "\titter: 3400\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -1.83139080e-12  -2.03006944e-07  -3.77780827e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -6.65468810e-19  -2.81733649e-06  -3.92649699e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00329219007912\n",
      "\tbest neuron used: 3\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 19, 19, 18, 23, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 13, 13, 19, 19, 19, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 23, 18, 19, 19, 19, 18, 18, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=61747: 11.8610951084\n",
      "time_step: 61748\n",
      "\titter: 3600\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.16915185e-08  -6.93792035e-09  -8.72522525e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -3.09143006e-17  -4.88366758e-08  -2.98399707e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000604430956839\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 61948\n",
      "\titter: 3800\n",
      "\tavg activation val: 0.08\n",
      "\tavg dw mean update: [ -1.76825252e-07  -2.78764707e-09  -2.05420559e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -3.88213165e-06  -1.65025818e-08  -4.31690541e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00117799625042\n",
      "\tbest neuron used: 24\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 62148\n",
      "\titter: 4000\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.13768321e-08  -7.21804713e-09  -8.58381567e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -3.01133236e-17  -5.25658402e-08  -2.90668292e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00120452281607\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 19, 19, 18, 23, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 13, 13, 19, 19, 19, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 23, 18, 19, 19, 19, 18, 18, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=62347: 11.8453927567\n",
      "time_step: 62348\n",
      "\titter: 4200\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.42053098e-11  -1.66145902e-08  -6.66578015e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.11527590e-11  -1.14567936e-07   0.00000000e+00 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00140416580383\n",
      "\tbest neuron used: 17\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 62548\n",
      "\titter: 4400\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.16096959e-08  -7.08067155e-09  -8.43359474e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.92437652e-17  -5.10479383e-08  -2.82274895e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00060393551547\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 62748\n",
      "\titter: 4600\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -9.77664133e-08  -1.47186810e-08  -1.63773406e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -5.07467517e-07  -3.71349386e-08  -9.06949573e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000722130443785\n",
      "\tbest neuron used: 19\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 23, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 19, 12, 18, 18, 23, 18, 18, 19, 19, 18, 13, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 13, 13, 19, 19, 19, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 23, 18, 19, 19, 19, 18, 18, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=62947: 11.8388063052\n",
      "time_step: 62948\n",
      "\titter: 4800\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.41562034e-11  -1.68322504e-08  -6.62244701e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.09040719e-11  -1.16978640e-07   0.00000000e+00 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00110117680869\n",
      "\tbest neuron used: 17\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 63148\n",
      "\titter: 5000\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.10667672e-08  -8.45297342e-09  -8.40082862e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.79443293e-17  -6.84554667e-08  -2.69732114e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000701400255244\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 63348\n",
      "\titter: 5200\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.34789528e-08  -8.37449584e-09  -8.32970877e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.75695989e-17  -6.75374864e-08  -2.66115036e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000600785853389\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 23, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 19, 19, 19, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 19, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 23, 18, 19, 19, 19, 19, 19, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=63547: 11.810752225\n",
      "time_step: 63548\n",
      "\titter: 5400\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.37294269e-08  -8.30826923e-09  -8.26523238e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.72570764e-17  -6.67718973e-08  -2.63098419e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000700402297712\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 63748\n",
      "\titter: 5600\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.35409817e-08  -8.22897835e-09  -8.19201457e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.68754313e-17  -6.58369780e-08  -2.59414596e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000699884090896\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 63948\n",
      "\titter: 5800\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.33707246e-08  -8.16014539e-09  -8.12088300e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.65402376e-17  -6.50158512e-08  -2.56179146e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000499793676372\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 23, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 18, 18, 19, 19, 18, 13, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 18, 18, 18, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 19, 19, 19, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 17, 17, 17, 17, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 23, 18, 19, 19, 19, 18, 18, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=64147: 11.8113063418\n",
      "time_step: 64148\n",
      "\titter: 6000\n",
      "\tavg activation val: 0.08\n",
      "\tavg dw mean update: [ -1.80855706e-07  -2.97649591e-09  -1.96916156e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -3.94136198e-06  -1.59996319e-08  -4.18533889e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00188082170105\n",
      "\tbest neuron used: 24\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 64348\n",
      "\titter: 6200\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.54922077e-11  -4.76609509e-08  -4.45006017e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.29585541e-16  -4.16578622e-08  -2.60799234e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00149534536105\n",
      "\tbest neuron used: 13\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 64548\n",
      "\titter: 6400\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -1.80097057e-12  -2.01521326e-07  -3.73422718e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -6.59473237e-19  -2.79195357e-06  -3.89112103e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00420504973607\n",
      "\tbest neuron used: 3\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 23, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 18, 18, 19, 19, 18, 13, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 17, 17, 17, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 19, 19, 19, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 17, 17, 17, 17, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 19, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 23, 18, 19, 19, 19, 18, 18, 19, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=64747: 11.8204862064\n",
      "time_step: 64748\n",
      "\titter: 6600\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.54424927e-11  -4.73839840e-08  -4.41749148e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.27879803e-16  -4.11095185e-08  -2.57366325e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.0010953760791\n",
      "\tbest neuron used: 13\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 64948\n",
      "\titter: 6800\n",
      "\tavg activation val: 0.1\n",
      "\tavg dw mean update: [ -1.07664428e-08  -8.54883005e-08  -4.69327755e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -5.63836447e-12  -5.67281192e-07  -6.06561360e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00383206943771\n",
      "\tbest neuron used: 14\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 65148\n",
      "\titter: 7000\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.26426410e-08  -7.77118859e-09  -8.35085019e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.47162013e-17  -6.05474935e-08  -2.87377143e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000499090847659\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 23, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 18, 18, 19, 19, 18, 13, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 17, 17, 17, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 19, 19, 19, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 17, 17, 17, 17, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 19, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 23, 18, 19, 19, 19, 18, 18, 23, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=65347: 11.8199443942\n",
      "time_step: 65348\n",
      "\titter: 7200\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.44805103e-08  -7.71270161e-09  -8.29137892e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.44433573e-17  -5.98791052e-08  -2.84204765e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000598995214284\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 65548\n",
      "\titter: 7400\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.43246506e-08  -7.65698232e-09  -8.23059267e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.41868257e-17  -5.92506774e-08  -2.81222052e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00059959787016\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 65748\n",
      "\titter: 7600\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.41331154e-08  -7.57693198e-09  -8.15216292e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.38028988e-17  -5.83101684e-08  -2.76758105e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00049864070456\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 23, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 18, 18, 19, 19, 18, 13, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 17, 17, 17, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 13, 13, 19, 19, 19, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 17, 17, 17, 17, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 19, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 23, 18, 19, 19, 19, 18, 18, 23, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=65947: 11.8038089089\n",
      "time_step: 65948\n",
      "\titter: 7800\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.41526255e-08  -7.50507919e-09  -8.07835335e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.34602321e-17  -5.74707348e-08  -2.72773893e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000497952513404\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 66148\n",
      "\titter: 8000\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.52300540e-11  -4.78708490e-08  -4.42260429e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.21199631e-16  -3.89620438e-08  -2.58190980e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.00119531739831\n",
      "\tbest neuron used: 13\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 66348\n",
      "\titter: 8200\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.37794110e-08  -8.15238570e-09  -7.92853659e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.27509155e-17  -6.56663368e-08  -2.64526616e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000496387985611\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18, 18, 18, 13, 13, 13, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 23, 18, 18, 18, 18, 13, 18, 23, 13, 13, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 19, 18, 18, 18, 18, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 23, 24, 24, 19, 19, 18, 18, 14, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 18, 18, 18, 18, 24, 24, 18, 18, 18, 19, 13, 13, 19, 19, 19, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 18, 18, 18, 18, 19, 19, 18, 18, 19, 19, 13, 13, 13, 18, 18, 18, 18, 18, 23, 23, 18, 14, 14, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 17, 17, 18, 18, 18, 13, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 24, 24, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 18, 18, 18, 18, 18, 18, 13, 13, 18, 19, 18, 18, 18, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 18, 18, 19, 19, 18, 13, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 13, 23, 23, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 18, 22, 22, 22, 18, 17, 17, 17, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 19, 19, 19, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 17, 17, 17, 17, 19, 19, 18, 18, 13, 24, 18, 18, 18, 18, 18, 18, 13, 18, 18, 23, 18, 18, 18, 19, 19, 18, 18, 23, 23, 23, 23, 23, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 18, 23, 23, 23, 17, 18, 18, 18, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 19, 18, 17, 18, 19, 18, 17, 18, 18, 18, 18, 13, 18, 18, 18, 24, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 12, 18, 18, 23, 23, 23, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 18, 18, 18, 14, 14, 18, 18, 18, 18, 18, 23, 18, 19, 19, 19, 18, 18, 23, 13, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 23, 23, 13, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 18, 18, 13, 23, 18, 17, 17, 19, 18, 18, 18, 18, 18, 18, 18, 13, 13, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 13, 13, 13, 13, 13, 13, 18]\n",
      "avg. quantization error at t=66547: 11.7774485105\n",
      "time_step: 66548\n",
      "\titter: 8400\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -2.36147124e-08  -8.07135337e-09  -7.85066271e-09 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -2.24054606e-17  -6.46692447e-08  -2.60509986e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000494893681284\n",
      "\tbest neuron used: 18\n",
      "hyperparams: lr=0.0001, var=1\n",
      "time_step: 66748\n",
      "\titter: 8600\n",
      "\tavg activation val: 0.12\n",
      "\tavg dw mean update: [ -1.51382246e-11  -4.81086738e-08  -4.37212429e-08 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tavg dw max update: [ -1.18041724e-16  -4.28380691e-08  -2.51463706e-07 ...,   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "\tsum max update: 0.000892812831612\n",
      "\tbest neuron used: 13\n",
      "hyperparams: lr=0.0001, var=1\n"
     ]
    }
   ],
   "source": [
    "neuron_dim = 5\n",
    "k_map_2 = KohonenMap(len(top_ad_ids), neuron_dim)\n",
    "\n",
    "# load the weights\n",
    "load_weights = np.genfromtxt('Data/KM_weights_J26_3.csv', delimiter=\",\")\n",
    "k_map_2.set_weights(load_weights)\n",
    "k_map_2.set_time_step(27000)\n",
    "\n",
    "# train\n",
    "k_map_2.train_dataset(clean_idx.values)\n",
    "\n",
    "# gen 1: lowest = 1.3\n",
    "# gen 2: lowest = 1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  8.28539052e-19   9.34513268e-03   2.40065341e-03 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " ..., \n",
      " [  2.48180140e-35   4.67761202e-03   2.64274040e-15 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  6.16553556e-03   7.56450955e-04   4.02793257e-09 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  3.94232069e-02   1.55244275e-04   4.06103030e-05 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# print weights at end\n",
    "print(k_map_2.weights)\n",
    "\n",
    "# save the weights of the Kohonen Map\n",
    "np.savetxt(\"Data/KM_weights_J26_4.csv\", k_map_2.weights, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg error: 12.1060325229\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhIAAAFkCAYAAAB1rtL+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8dcef0bed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### graph the quant error over time of the map\n",
    "error_vals = k_map_2.error_over_time\n",
    "print(\"avg error: {}\".format(np.sum(error_vals) / len(error_vals)))\n",
    "plt.plot(range(0, len(error_vals)), error_vals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTES:**: \n",
    "5 (1) epochs (5% data), 5x5 latice, 10 = v_0, 1 = lr_0, decrease mono to 1000, 0.1 = min_lr, 1.0 = min_v ===> 0.1 min. qer @ t = 6000\n",
    "\n",
    "2 (?) epochs (5% data), 10x10 latice, 10 = v_0, 1 = lr_0, decrease mono to 1000, 0.1 = min_lr, 1.0 = min_v ===> 0.1 min. qer @ t = 6000\n",
    "\n",
    "2 (?) epochs (20% data), 5x5 latice, 10 = v_0, 1 = lr_0, decrease mono to 1000, 0.1 = min_lr, 1.0 = min_v ===> 1.1 min. qer @ t = NO REAL IMPROVEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting weights: \n",
      "[[ 0.  0.  0. ...,  1.  1.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  1. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "starting weights: \n",
      "[[  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  8.28539052e-19   9.34513268e-03   2.40065341e-03 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " ..., \n",
      " [  2.48180140e-35   4.67761202e-03   2.64274040e-15 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  6.16553556e-03   7.56450955e-04   4.02793257e-09 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  3.94232069e-02   1.55244275e-04   4.06103030e-05 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]]\n",
      "START: avg. quantixation error calc\n",
      "avg error: 12.9989628824\n",
      "best_neurons: {0: 1, 3: 15, 4: 34, 8: 21, 9: 37, 12: 2, 13: 279, 14: 115, 17: 229, 18: 1593, 19: 257, 22: 95, 23: 238, 24: 84}\n"
     ]
    }
   ],
   "source": [
    "# load the trained map for testing\n",
    "k_map_analyse = KohonenMap(len(top_ad_ids), neuron_dim)\n",
    "load_weights = np.genfromtxt('Data/KM_weights_J26_4.csv', delimiter=\",\")\n",
    "k_map_analyse.set_weights(load_weights)\n",
    "\n",
    "avg_error, best_neurons = k_map_analyse.average_quantization_error(clean_idx.values, rand=False, count=3000)\n",
    "\n",
    "print('avg error: {}'.format(avg_error))\n",
    "print('best_neurons: {}'.format(best_neurons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAFyCAYAAAD4aN2QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlfv/aNQgrKR5gEYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8da7c5be10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create heat map of the popularity of each neuron\n",
    "neuron_latice = np.zeros(shape=(neuron_dim, neuron_dim))\n",
    "sum_latice = 0\n",
    "for n in best_neurons:\n",
    "    sum_latice += best_neurons[n]\n",
    "\n",
    "for n in best_neurons:\n",
    "    x_pos, y_pos = k_map_analyse.get_neuron_position(n)\n",
    "    neuron_latice[x_pos, y_pos] = float(best_neurons[n]) / float(sum_latice)\n",
    "    \n",
    "plt.imshow(neuron_latice, cmap='hot', interpolation='nearest')\n",
    "plt.title('popularity of neurons on 3000 input vectors')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting weights: \n",
      "[[ 2.05705722  0.56625404  1.7568975  ...,  0.14773438  1.15054697\n",
      "  -0.85156475]\n",
      " [-0.80235895  1.01590535 -0.22496112 ...,  1.89909514 -0.25043556\n",
      "   0.3335187 ]\n",
      " [ 0.37020456 -0.08609844  0.11765797 ..., -0.5228479   0.15090678\n",
      "  -0.95699605]\n",
      " ..., \n",
      " [ 0.12095395  0.25358958 -0.54357809 ...,  0.6693518  -1.87111955\n",
      "   0.08355185]\n",
      " [ 1.03287594 -1.13357871 -0.24696651 ..., -2.17794938  0.22025854\n",
      "  -0.39764427]\n",
      " [ 0.72350305 -2.2060565  -0.43368212 ..., -1.76297636 -0.23794573\n",
      "   0.14884992]]\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "START: avg. quantixation error calc\n",
      "best neurons: [17, 18, 18, 18, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 15, 15, 17, 17, 17, 20, 20, 20, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 20, 20, 20, 20, 20, 17, 18, 18, 18, 17, 17, 17, 17, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 7, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 12, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 20, 20, 20, 20, 20, 20, 20, 17, 17, 17, 17, 17, 17, 17, 17, 17, 24, 24, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 22, 22, 22, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 12, 12, 17, 17, 17, 18, 18, 17, 11, 17, 17, 17, 22, 17, 17, 22, 22, 18, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 19, 19, 17, 17, 0, 0, 17, 17, 17, 17, 17, 17, 17, 12, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 12, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 11, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 17, 17, 17, 17, 17, 17, 17, 17, 20, 17, 17, 17, 17, 17, 17, 17, 17, 17, 20, 20, 17, 17, 17, 20, 20, 20, 20, 20, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 24, 24, 24, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 20, 20, 17, 17, 17, 17, 17, 17, 17, 15, 15, 12, 17, 15, 15, 18, 18, 18, 18, 17, 17, 17, 17, 17, 20, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 15, 15, 17, 17, 20, 20, 20, 20, 20, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 24, 24, 24, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 21, 17, 17, 15, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 15, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 0, 0, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 13, 20, 17, 17, 17, 17, 20, 20, 17, 17, 18, 17, 12, 12, 12, 12, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 20, 20, 17, 20, 20, 17, 20, 20, 20, 17, 17, 20, 20, 20, 20, 20, 17, 17, 20, 20, 17, 22, 22, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 20, 20, 20, 20, 20, 17, 17, 22, 17, 17, 20, 20, 20, 20, 20, 20, 17, 17, 17, 17, 4, 4, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 19, 22, 17, 8, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 15, 2, 2, 17, 17, 17, 10, 10, 10, 10, 10, 20, 20, 20, 20, 20, 20, 17, 17, 24, 24, 24, 24, 20, 20, 20, 20, 20, 15, 15, 17, 17, 17, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 20, 20, 20, 20, 20, 17, 17, 24, 24, 24, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 15, 15, 15, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 17, 17, 17, 19, 19, 17, 17, 7, 7, 7, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 15, 15, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 14, 14, 14, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 20, 20, 22, 22, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 2, 2, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 17, 17, 17, 18, 17, 17, 17, 17, 17, 17, 17, 15, 15, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 21, 21, 21, 21, 17, 17, 17, 17, 17, 17, 17, 17, 15, 15, 15, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 12, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 17, 17, 17, 17, 15, 15, 15, 15, 17, 17, 17, 17, 17, 17, 17, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 22, 22, 17, 17, 17, 17, 17, 17, 22, 22, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 12, 17, 17, 17, 17, 17, 18, 17, 17, 12, 11, 11, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 12, 12, 12, 17, 17, 17, 17, 17, 17, 13, 13, 13, 13, 13, 13, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 0, 0, 0, 0, 17, 17, 17, 17, 8, 8, 17, 17, 17, 17, 17, 17, 20, 20, 17, 17, 17, 17, 17, 17, 17, 18, 11, 11, 11, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 22, 22, 17, 17, 17, 17, 17, 17, 17, 17, 22, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 17, 17, 17, 17, 17, 17, 17, 11, 17, 17, 17, 17, 17, 17, 17, 17, 17, 11, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 12, 12, 12, 17, 17, 17, 17, 17, 17, 17, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 17, 17, 17, 17, 17, 17, 15, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 10, 10, 10, 17, 18, 17, 17, 17, 17, 17, 17, 15, 15, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 10, 10, 10, 10, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 12, 17, 17, 17, 17, 17, 10, 10, 10, 12, 12, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 10, 10, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 17, 17, 17, 10, 10, 10, 10, 17, 17, 17, 17, 17, 10, 10, 10, 10, 17, 17, 17, 10, 10, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 17, 17, 22, 22, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 15, 15, 15, 15, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 20, 20, 20, 20, 20, 17, 17, 11, 11, 17, 17, 17, 17, 17, 17, 17, 17, 12, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 24, 24, 17, 17, 17, 17, 17, 17, 17, 17, 17, 11, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 11, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 24, 17, 17, 17, 17, 17, 18, 14, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 11, 11, 12, 12, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 21, 21, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 12, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 15, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 10, 17, 17, 12, 12, 18, 18, 18, 11, 11, 11, 17, 17, 15, 15, 15, 15, 15, 15, 15, 8, 8, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 11, 11, 11, 11, 17, 17, 17, 17, 17, 17, 17, 24, 24, 24, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 12, 12, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 22, 22, 22, 17, 24, 24, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 17, 17, 18, 18, 17, 17, 17, 17, 17, 17, 17, 17, 11, 11, 11, 11, 24, 24, 24, 17, 17, 17, 17, 17, 17, 24, 24, 24, 24, 22, 22, 22, 22, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 15, 15, 11, 17, 17, 17, 17, 17, 17, 17, 17, 14, 17, 17, 17, 17, 17, 20, 20, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 22, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 12, 12, 12, 12, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 4, 4, 4, 4, 17, 17, 18, 18, 17, 17, 17, 17, 17, 17, 17, 17, 14, 14, 14, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 22, 17, 17, 17, 17, 24, 17, 17, 17, 17, 18, 18, 17, 17, 17, 17, 17, 17, 18, 24, 24, 24, 24, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 3, 3, 17, 17, 17, 15, 15, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 22, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 11, 11, 17, 17, 17, 17, 17, 22, 17, 2, 2, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 14, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 14, 14, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 14, 14, 14, 14, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 17, 17, 24, 24, 24, 9, 9, 9, 9, 17, 17, 18, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 14, 17, 17, 17, 17, 17, 17, 22, 22, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 8, 8, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 8, 8, 17, 17, 10, 10, 10, 17, 17, 22, 17, 17, 22, 2, 2, 2, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 11, 11, 15, 15, 17, 17, 17, 17, 17, 17, 18, 18, 17, 17, 9, 9, 9, 9, 9, 9, 17, 17, 17, 17, 2, 2, 18, 17, 17, 17, 17, 17, 17, 17, 17, 12, 17, 17, 17, 18, 18, 17, 17, 11, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 8, 8, 17, 17, 17, 17, 17, 17, 17, 12, 12, 15, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 17, 17, 17, 17, 17, 18, 17, 17, 18, 18, 17, 17, 17, 17, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 17, 17, 3, 3, 3, 3, 17, 17, 17, 17, 17, 18, 17, 17, 14, 18, 16, 17, 17, 17, 17, 17, 18, 17, 17, 11, 24, 24, 24, 24, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 6, 6, 6, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 14, 17, 12, 12, 17, 17, 17, 17, 17, 17, 17, 17, 24, 24, 17, 17, 17, 17, 17, 17, 17, 17, 22, 17, 17, 17, 17, 17, 17, 17, 17, 5, 5, 5, 1, 1, 17, 17, 17, 17, 2, 2, 2, 2, 17, 17, 17, 10, 10, 2, 2, 2, 17, 17, 18, 10, 10, 10, 10, 10, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 10, 10, 10, 10, 17, 17, 17, 0, 0, 0, 0, 0, 18, 17, 14, 14, 14, 6, 6, 6, 6, 10, 10, 10, 17, 17, 17, 17, 12, 17, 17, 12, 12, 12, 12, 17, 22, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 11, 11, 11, 17, 2, 2, 17, 17, 17, 17, 17, 17, 15, 17, 17, 2, 2, 17, 17, 17, 2, 2, 2, 17, 17, 12, 17, 17, 17, 17, 6, 6, 6, 6, 6, 17, 17, 6, 17, 6, 6, 17, 17, 17, 17, 17, 6, 6, 17, 17, 17, 17, 8, 17, 17, 2, 2, 2, 2, 2, 18, 8, 6, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 17, 17, 17, 17, 6, 6, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 12, 7, 7, 7, 7, 7, 6, 6, 20, 6, 6, 6, 8, 8, 6, 6, 17, 17, 17, 6, 6, 6, 17, 17, 17, 24, 24, 24, 6, 6, 6, 6, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 21, 17, 20, 17, 11, 11, 12, 17, 17, 12, 15, 15, 15, 17, 17, 17, 17, 15, 17, 17, 17, 17, 17, 17, 17, 17, 12, 17, 17, 17, 17, 17, 17, 22, 17, 22, 22, 17, 17, 17, 17, 17, 17, 17, 14, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 19, 19, 19, 19, 19, 19, 17, 17, 17, 17, 17, 17, 18, 17, 19, 19, 19, 8, 8, 8, 17, 17, 17, 17, 19, 19, 19, 19, 6, 17, 17, 17, 17, 19, 19, 17, 21, 21, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 19, 19, 17, 17, 17, 17, 17, 17, 8, 17, 17, 19, 19, 19, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 19, 19, 17, 17, 19, 19, 19, 17, 17, 8, 8, 17, 17, 17, 19, 17, 17, 17, 17, 17, 8, 21, 18, 17, 14, 14, 14, 14, 8, 8, 8, 8, 17, 17, 17, 24, 24, 12, 17, 17, 17, 17, 17, 19, 20, 20, 17, 17, 17, 11, 11, 17, 17, 8, 20, 20, 17, 17, 17, 17, 17, 17, 17, 12, 12, 18, 18, 18, 17, 17, 12, 12, 17, 17, 17, 18, 18, 17, 17, 17, 17, 17, 8, 18, 18, 18, 18, 18, 17, 17, 8, 8, 17, 17, 17, 17, 17, 17, 24, 24, 24, 17, 17, 17, 17, 12, 17, 17, 17, 18, 18, 24, 24, 17, 17, 17, 17, 17, 17, 17, 17, 17, 8, 8, 8, 8, 17, 17, 17, 24, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 14, 14, 14, 24, 24, 2, 2, 9, 9, 9, 9, 9, 17, 17, 4, 4, 4, 4, 4, 4, 24, 24, 24, 24, 24, 24, 8, 8, 18, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 17, 17, 17, 18, 18, 15, 17, 18, 18, 8, 8, 8, 18, 17, 17, 17, 24, 24, 24, 17, 17, 17, 17, 18, 17, 17, 18, 18, 18, 17, 17, 17, 12, 12, 12, 12, 17, 18, 18, 18, 18, 18, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 8, 8, 8, 17, 17, 17, 8]\n",
      "avg. quant. error random: 28685.0425751\n",
      "avg. quant. error train: 0.0508642678345\n"
     ]
    }
   ],
   "source": [
    "# avg quantization error of entire dataset\n",
    "k_base = KohonenMap(len(top_ad_ids), neuron_dim)\n",
    "# base_aq = k_base.average_quantization_error(clean_idx.values, rand=False, count=3500)\n",
    "\n",
    "avg_error = k_map_2.average_quantization_error(clean_idx.values, rand=False, count=3500)\n",
    "# avg_error = 0\n",
    "# for i in range(0, 10):\n",
    "#     clear_output()\n",
    "#     avg_error += k_map.average_quantization_error(clean_idx.values)\n",
    "\n",
    "print(\"avg. quant. error random: {}\".format(base_aq))\n",
    "print(\"avg. quant. error train: {}\".format(avg_error / 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Recommend ads for the test users: ** we now run the user test data through the map and find the best neurons. We compare the weights of the neurons to that of the test user's weights and any vector features > 0.5 (TODO: test this threshold) we add that ad feature to the ones we would recommend for that user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'user_ad_data_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-c6408fb0ad2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m29\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mn_best\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk_map_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_neuron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser_ad_data_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mn_best_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk_map_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_best\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# get the difference between the two weights:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'user_ad_data_test' is not defined"
     ]
    }
   ],
   "source": [
    "# user_ad_data_test\n",
    "for i in range(0, 1):\n",
    "    idx = 29\n",
    "    n_best = k_map_2.get_best_neuron(np.asarray([user_ad_data_test[idx, :]]))\n",
    "    n_best_weights = k_map_2.weights[n_best, :]\n",
    "    # get the difference between the two weights:\n",
    "    print(\"user recommends: {}\".format(np.sum(user_ad_data_test[idx, :])))\n",
    "    n_to_user_diff = n_best_weights - user_ad_data_test[idx, :]\n",
    "    n_to_user_diff_recommendations = (n_to_user_diff > 0).astype(int)\n",
    "    print(\"diff: {}\".format(np.sum(n_to_user_diff_recommendations)))\n",
    "    print(\"best neuron: {}\".format(n_best))\n",
    "    print(n_to_user_diff_recommendations[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## [PAGE BREAK] ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total messages sent: 4067494.0\n"
     ]
    }
   ],
   "source": [
    "#how many total messages were sent by all users\n",
    "numMessagesTotal = np.nansum(user_dt['ad_messages'].values)\n",
    "print(\"total messages sent: {}\".format(numMessagesTotal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiYAAAFkCAYAAAAUtvC8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxERERESy8f87xIlPFRIleAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa19cd5bf10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # ad id vs messages sent\n",
    "# ad_ids_unique = user_dt['ad_id'].unique()\n",
    "# ad_ids_unique_count = np.zeros(np.shape(ad_ids_unique))\n",
    "# # print(ad_ids_unique[0])\n",
    "# # print(np.where(user_dt['ad_id'] == ad_ids_unique[0]))\n",
    "# # print(user_dt['ad_messages'].values)\n",
    "# # print(user_dt['ad_messages'].values[np.where(user_dt['ad_id'] == ad_ids_unique[0])])\n",
    "# for i in range(0, len(ad_ids_unique)):\n",
    "#     ad_ids_unique_count[i] = sum(user_dt['ad_messages'].values[np.where(user_dt['ad_id'] == ad_ids_unique[0])])\n",
    "    \n",
    "ad_ids = user_dt['ad_id']\n",
    "user_messages = user_dt['ad_messages']\n",
    "plt.scatter(ad_ids, user_messages)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAFyCAYAAAAnENp+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa19ce73a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# distribution of ads chosen by user during the post-7-day period\n",
    "# ads_ = np.zeros(1)\n",
    "# for a in user_ad_interests_dt['ads'].values:\n",
    "#     print(a)\n",
    "#     print(ads_)\n",
    "#     ads_ = np.hstack((ads_, a))\n",
    "ads_ = sum(user_ad_interests_dt['ads'].apply(eval).tolist(), [])\n",
    "hist, bins = np.histogram(ads_, bins=1000)\n",
    "width = 0.7 * (bins[1] - bins[0])\n",
    "center = (bins[:-1] + bins[1:]) / 2\n",
    "plt.bar(center, hist, align='center', width=width)\n",
    "plt.title('distribution of ads messaged by users in 7-day-post period')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
